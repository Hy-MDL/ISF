{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b2fe4b-de53-4f7b-916b-20deaca82910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d9ff9-3a19-4370-af9a-788c987c5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home1/gkrtod35/Diffusion-TS/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88dcbe7e-1aa5-4d8f-ba6e-03e02a5e255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idx</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>solar generation</th>\n",
       "      <th>일시</th>\n",
       "      <th>기온(°C)</th>\n",
       "      <th>풍속(m/s)</th>\n",
       "      <th>풍향(16방위)</th>\n",
       "      <th>습도(%)</th>\n",
       "      <th>증기압(hPa)</th>\n",
       "      <th>...</th>\n",
       "      <th>일조(hr)</th>\n",
       "      <th>일사(MJ/m2)</th>\n",
       "      <th>적설(cm)</th>\n",
       "      <th>전운량(10분위)</th>\n",
       "      <th>중하층운량(10분위)</th>\n",
       "      <th>지면온도(°C)</th>\n",
       "      <th>5cm 지중온도(°C)</th>\n",
       "      <th>10cm 지중온도(°C)</th>\n",
       "      <th>20cm 지중온도(°C)</th>\n",
       "      <th>30cm 지중온도(°C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 00:00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 01:00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>250.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 02:00</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>250.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 03:00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 04:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>270.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idx        date  time  solar generation                일시  기온(°C)  풍속(m/s)  \\\n",
       "0    0  2014-01-01     0               0.0  2014-01-01 00:00     3.3      3.8   \n",
       "1    0  2014-01-01     1               0.0  2014-01-01 01:00     2.6      2.3   \n",
       "2    0  2014-01-01     2               0.0  2014-01-01 02:00     1.7      1.7   \n",
       "3    0  2014-01-01     3               0.0  2014-01-01 03:00     1.4      1.4   \n",
       "4    0  2014-01-01     4               0.0  2014-01-01 04:00     0.9      2.8   \n",
       "\n",
       "   풍향(16방위)  습도(%)  증기압(hPa)  ...  일조(hr)  일사(MJ/m2)  적설(cm)  전운량(10분위)  \\\n",
       "0     250.0   65.0       5.0  ...     0.0        0.0     0.0        6.0   \n",
       "1     250.0   66.0       4.9  ...     0.0        0.0     0.0        0.0   \n",
       "2     250.0   67.0       4.6  ...     0.0        0.0     0.0        0.0   \n",
       "3     250.0   60.0       4.1  ...     0.0        0.0     0.0        0.0   \n",
       "4     270.0   59.0       3.8  ...     0.0        0.0     0.0        0.0   \n",
       "\n",
       "   중하층운량(10분위)  지면온도(°C)  5cm 지중온도(°C)  10cm 지중온도(°C)  20cm 지중온도(°C)  \\\n",
       "0          6.0       0.0           0.1           -0.2            0.0   \n",
       "1          0.0      -0.1           0.1           -0.2            0.1   \n",
       "2          0.0      -0.3           0.0           -0.2            0.0   \n",
       "3          0.0      -0.4           0.0           -0.2            0.1   \n",
       "4          0.0      -0.6           0.0           -0.2            0.0   \n",
       "\n",
       "   30cm 지중온도(°C)  \n",
       "0            1.5  \n",
       "1            1.5  \n",
       "2            1.5  \n",
       "3            1.5  \n",
       "4            1.5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merged_data_processed_seoul.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3216fd-0275-4a0e-88c0-f99a03af6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seq_len  = 8760   # 학습용: 1년치 (1h 단위 → 8760h)\n",
    "pred_len =   24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df = df[-(seq_len + pred_len) : ]  # shape (8760, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22dfead4-8f64-407e-9c1a-fe25aa32fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df = numeric_df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4af6afc-29ea-413b-b0a6-c02552b030b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=720, pred_len=24, stride=24):\n",
    "        \"\"\"\n",
    "        data: array-like [T, C]\n",
    "        seq_len: 입력 길이 (예: 720)\n",
    "        pred_len: 예측 길이 (예: 24)\n",
    "        stride: 윈도우 이동 간격 (예: 1)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        T, C  = data.shape\n",
    "        self.X, self.Y = [], []\n",
    "        for i in range(0, T - seq_len - pred_len + 1, stride):\n",
    "            self.X.append(data[i : i+seq_len])                          # [seq_len, C]\n",
    "            self.Y.append(data[i+seq_len : i+seq_len+pred_len, 0])      # [pred_len]\n",
    "        self.X = torch.from_numpy(np.stack(self.X))  # [N, seq_len, C]\n",
    "        self.Y = torch.from_numpy(np.stack(self.Y))  # [N, pred_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # [C, seq_len], [pred_len]\n",
    "        return self.X[idx].transpose(0,1), self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6332d186-9f41-435c-8256-196d13b84f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의: sliding window로 (X, y) 생성\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=8760, pred_len=24):\n",
    "        \"\"\"\n",
    "        data: NumPy array of shape [T, C] (시간 T, 채널 C)\n",
    "        seq_len: 입력 시퀀스 길이 (8760)\n",
    "        pred_len: 예측할 시퀀스 길이 (24)\n",
    "        \"\"\"\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        \n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        T, C = data.shape\n",
    "        for i in range(T - seq_len - pred_len + 1):\n",
    "            self.X.append(data[i:i+seq_len])\n",
    "            self.Y.append(data[i+seq_len:i+seq_len+pred_len, 0])  # 채널0=solar 예측\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)      # [N, seq_len, C]\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.float32)      # [N, pred_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 모델에 넣기 편하게 채널 차원을 앞쪽으로 옮김: [C, seq_len]\n",
    "        return self.X[idx].transpose(0,1), self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15f8a48-2b45-4e54-ae94-63d61460a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의: CNN → LSTM → FC\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, cnn_channels=32, lstm_hidden=64, pred_len=24):\n",
    "        super().__init__()\n",
    "        # 1D-CNN: 채널별 시계열 특성 추출\n",
    "        self.conv1 = nn.Conv1d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.pool  = nn.MaxPool1d(2)  # 길이 절반\n",
    "        \n",
    "        # LSTM: 시계열 의존성 학습\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # 최종 24시간 예측용 FC\n",
    "        self.fc = nn.Linear(lstm_hidden, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, seq_len]\n",
    "        x = self.relu(self.conv1(x))     # [B, 16, L]\n",
    "        x = self.relu(self.conv2(x))     # [B, cnn_channels, L]\n",
    "        x = self.pool(x)                 # [B, cnn_channels, L/2]\n",
    "        \n",
    "        # LSTM 입력: (batch, time, feat)\n",
    "        x = x.transpose(1,2)             # [B, L/2, cnn_channels]\n",
    "        out, _ = self.lstm(x)            # out: [B, L/2, lstm_hidden]\n",
    "        \n",
    "        # 마지막 시점만 사용해 예측\n",
    "        out = out[:, -1, :]              # [B, lstm_hidden]\n",
    "        y_hat = self.fc(out)             # [B, pred_len]\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c86f88c-fd27-4808-8349-903c6dca4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/평가 루프\n",
    "def train(model, loader, optim, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            total_loss += criterion(y_pred, y).item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5ba7c4d-f172-4296-ba94-c3c9aee22b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.0631 | Val Loss: 0.0218\n",
      "Epoch 02 | Train Loss: 0.0197 | Val Loss: 0.0180\n",
      "Epoch 03 | Train Loss: 0.0185 | Val Loss: 0.0170\n",
      "Epoch 04 | Train Loss: 0.0172 | Val Loss: 0.0167\n",
      "Epoch 05 | Train Loss: 0.0170 | Val Loss: 0.0170\n",
      "Epoch 06 | Train Loss: 0.0168 | Val Loss: 0.0171\n",
      "Epoch 07 | Train Loss: 0.0162 | Val Loss: 0.0165\n",
      "Epoch 08 | Train Loss: 0.0164 | Val Loss: 0.0166\n",
      "Epoch 09 | Train Loss: 0.0159 | Val Loss: 0.0164\n",
      "Epoch 10 | Train Loss: 0.0157 | Val Loss: 0.0202\n",
      "Epoch 11 | Train Loss: 0.0169 | Val Loss: 0.0183\n",
      "Epoch 12 | Train Loss: 0.0163 | Val Loss: 0.0184\n",
      "Epoch 13 | Train Loss: 0.0150 | Val Loss: 0.0180\n",
      "Epoch 14 | Train Loss: 0.0154 | Val Loss: 0.0171\n",
      "Epoch 15 | Train Loss: 0.0147 | Val Loss: 0.0176\n",
      "Epoch 16 | Train Loss: 0.0147 | Val Loss: 0.0170\n",
      "Epoch 17 | Train Loss: 0.0149 | Val Loss: 0.0172\n",
      "Epoch 18 | Train Loss: 0.0152 | Val Loss: 0.0185\n",
      "Epoch 19 | Train Loss: 0.0142 | Val Loss: 0.0177\n",
      "Epoch 20 | Train Loss: 0.0139 | Val Loss: 0.0176\n",
      "Epoch 21 | Train Loss: 0.0136 | Val Loss: 0.0180\n",
      "Epoch 22 | Train Loss: 0.0141 | Val Loss: 0.0190\n",
      "Epoch 23 | Train Loss: 0.0152 | Val Loss: 0.0174\n",
      "Epoch 24 | Train Loss: 0.0140 | Val Loss: 0.0185\n",
      "Epoch 25 | Train Loss: 0.0139 | Val Loss: 0.0177\n",
      "Epoch 26 | Train Loss: 0.0130 | Val Loss: 0.0188\n",
      "Epoch 27 | Train Loss: 0.0126 | Val Loss: 0.0184\n",
      "Epoch 28 | Train Loss: 0.0130 | Val Loss: 0.0185\n",
      "Epoch 29 | Train Loss: 0.0129 | Val Loss: 0.0193\n",
      "Epoch 30 | Train Loss: 0.0129 | Val Loss: 0.0191\n",
      "Epoch 31 | Train Loss: 0.0123 | Val Loss: 0.0187\n",
      "Epoch 32 | Train Loss: 0.0127 | Val Loss: 0.0200\n",
      "Epoch 33 | Train Loss: 0.0126 | Val Loss: 0.0209\n",
      "Epoch 34 | Train Loss: 0.0126 | Val Loss: 0.0203\n",
      "Epoch 35 | Train Loss: 0.0136 | Val Loss: 0.0208\n",
      "Epoch 36 | Train Loss: 0.0140 | Val Loss: 0.0228\n",
      "Epoch 37 | Train Loss: 0.0130 | Val Loss: 0.0192\n",
      "Epoch 38 | Train Loss: 0.0126 | Val Loss: 0.0202\n",
      "Epoch 39 | Train Loss: 0.0121 | Val Loss: 0.0203\n",
      "Epoch 40 | Train Loss: 0.0119 | Val Loss: 0.0209\n",
      "Epoch 41 | Train Loss: 0.0118 | Val Loss: 0.0220\n",
      "Epoch 42 | Train Loss: 0.0118 | Val Loss: 0.0202\n",
      "Epoch 43 | Train Loss: 0.0118 | Val Loss: 0.0207\n",
      "Epoch 44 | Train Loss: 0.0114 | Val Loss: 0.0213\n",
      "Epoch 45 | Train Loss: 0.0108 | Val Loss: 0.0217\n",
      "Epoch 46 | Train Loss: 0.0118 | Val Loss: 0.0220\n",
      "Epoch 47 | Train Loss: 0.0109 | Val Loss: 0.0222\n",
      "Epoch 48 | Train Loss: 0.0112 | Val Loss: 0.0226\n",
      "Epoch 49 | Train Loss: 0.0103 | Val Loss: 0.0238\n",
      "Epoch 50 | Train Loss: 0.0108 | Val Loss: 0.0254\n",
      "Epoch 51 | Train Loss: 0.0110 | Val Loss: 0.0229\n",
      "Epoch 52 | Train Loss: 0.0107 | Val Loss: 0.0234\n",
      "Epoch 53 | Train Loss: 0.0115 | Val Loss: 0.0248\n",
      "Epoch 54 | Train Loss: 0.0106 | Val Loss: 0.0216\n",
      "Epoch 55 | Train Loss: 0.0106 | Val Loss: 0.0216\n",
      "Epoch 56 | Train Loss: 0.0103 | Val Loss: 0.0230\n",
      "Epoch 57 | Train Loss: 0.0107 | Val Loss: 0.0221\n",
      "Epoch 58 | Train Loss: 0.0104 | Val Loss: 0.0237\n",
      "Epoch 59 | Train Loss: 0.0102 | Val Loss: 0.0216\n",
      "Epoch 60 | Train Loss: 0.0100 | Val Loss: 0.0246\n",
      "Epoch 61 | Train Loss: 0.0104 | Val Loss: 0.0224\n",
      "Epoch 62 | Train Loss: 0.0099 | Val Loss: 0.0236\n",
      "Epoch 63 | Train Loss: 0.0093 | Val Loss: 0.0238\n",
      "Epoch 64 | Train Loss: 0.0098 | Val Loss: 0.0251\n",
      "Epoch 65 | Train Loss: 0.0105 | Val Loss: 0.0221\n",
      "Epoch 66 | Train Loss: 0.0097 | Val Loss: 0.0247\n",
      "Epoch 67 | Train Loss: 0.0091 | Val Loss: 0.0236\n",
      "Epoch 68 | Train Loss: 0.0095 | Val Loss: 0.0227\n",
      "Epoch 69 | Train Loss: 0.0094 | Val Loss: 0.0242\n",
      "Epoch 70 | Train Loss: 0.0095 | Val Loss: 0.0237\n",
      "Epoch 71 | Train Loss: 0.0089 | Val Loss: 0.0250\n",
      "Epoch 72 | Train Loss: 0.0088 | Val Loss: 0.0238\n",
      "Epoch 73 | Train Loss: 0.0094 | Val Loss: 0.0239\n",
      "Epoch 74 | Train Loss: 0.0086 | Val Loss: 0.0252\n",
      "Epoch 75 | Train Loss: 0.0087 | Val Loss: 0.0261\n",
      "Epoch 76 | Train Loss: 0.0121 | Val Loss: 0.0184\n",
      "Epoch 77 | Train Loss: 0.0116 | Val Loss: 0.0234\n",
      "Epoch 78 | Train Loss: 0.0096 | Val Loss: 0.0226\n",
      "Epoch 79 | Train Loss: 0.0098 | Val Loss: 0.0260\n",
      "Epoch 80 | Train Loss: 0.0103 | Val Loss: 0.0216\n",
      "Epoch 81 | Train Loss: 0.0098 | Val Loss: 0.0246\n",
      "Epoch 82 | Train Loss: 0.0096 | Val Loss: 0.0224\n",
      "Epoch 83 | Train Loss: 0.0081 | Val Loss: 0.0241\n",
      "Epoch 84 | Train Loss: 0.0080 | Val Loss: 0.0256\n",
      "Epoch 85 | Train Loss: 0.0085 | Val Loss: 0.0222\n",
      "Epoch 86 | Train Loss: 0.0082 | Val Loss: 0.0235\n",
      "Epoch 87 | Train Loss: 0.0077 | Val Loss: 0.0259\n",
      "Epoch 88 | Train Loss: 0.0078 | Val Loss: 0.0248\n",
      "Epoch 89 | Train Loss: 0.0079 | Val Loss: 0.0259\n",
      "Epoch 90 | Train Loss: 0.0083 | Val Loss: 0.0216\n",
      "Epoch 91 | Train Loss: 0.0087 | Val Loss: 0.0264\n",
      "Epoch 92 | Train Loss: 0.0100 | Val Loss: 0.0234\n",
      "Epoch 93 | Train Loss: 0.0101 | Val Loss: 0.0219\n",
      "Epoch 94 | Train Loss: 0.0079 | Val Loss: 0.0260\n",
      "Epoch 95 | Train Loss: 0.0072 | Val Loss: 0.0259\n",
      "Epoch 96 | Train Loss: 0.0067 | Val Loss: 0.0258\n",
      "Epoch 97 | Train Loss: 0.0074 | Val Loss: 0.0228\n",
      "Epoch 98 | Train Loss: 0.0074 | Val Loss: 0.0246\n",
      "Epoch 99 | Train Loss: 0.0072 | Val Loss: 0.0238\n",
      "Epoch 100 | Train Loss: 0.0076 | Val Loss: 0.0242\n",
      "Epoch 101 | Train Loss: 0.0071 | Val Loss: 0.0238\n",
      "Epoch 102 | Train Loss: 0.0064 | Val Loss: 0.0256\n",
      "Epoch 103 | Train Loss: 0.0064 | Val Loss: 0.0231\n",
      "Epoch 104 | Train Loss: 0.0064 | Val Loss: 0.0244\n",
      "Epoch 105 | Train Loss: 0.0062 | Val Loss: 0.0252\n",
      "Epoch 106 | Train Loss: 0.0058 | Val Loss: 0.0230\n",
      "Epoch 107 | Train Loss: 0.0058 | Val Loss: 0.0255\n",
      "Epoch 108 | Train Loss: 0.0059 | Val Loss: 0.0232\n",
      "Epoch 109 | Train Loss: 0.0057 | Val Loss: 0.0258\n",
      "Epoch 110 | Train Loss: 0.0065 | Val Loss: 0.0252\n",
      "Epoch 111 | Train Loss: 0.0060 | Val Loss: 0.0243\n",
      "Epoch 112 | Train Loss: 0.0067 | Val Loss: 0.0242\n",
      "Epoch 113 | Train Loss: 0.0063 | Val Loss: 0.0289\n",
      "Epoch 114 | Train Loss: 0.0061 | Val Loss: 0.0261\n",
      "Epoch 115 | Train Loss: 0.0050 | Val Loss: 0.0235\n",
      "Epoch 116 | Train Loss: 0.0050 | Val Loss: 0.0245\n",
      "Epoch 117 | Train Loss: 0.0047 | Val Loss: 0.0282\n",
      "Epoch 118 | Train Loss: 0.0050 | Val Loss: 0.0243\n",
      "Epoch 119 | Train Loss: 0.0087 | Val Loss: 0.0245\n",
      "Epoch 120 | Train Loss: 0.0119 | Val Loss: 0.0192\n",
      "Epoch 121 | Train Loss: 0.0086 | Val Loss: 0.0238\n",
      "Epoch 122 | Train Loss: 0.0061 | Val Loss: 0.0257\n",
      "Epoch 123 | Train Loss: 0.0057 | Val Loss: 0.0239\n",
      "Epoch 124 | Train Loss: 0.0050 | Val Loss: 0.0248\n",
      "Epoch 125 | Train Loss: 0.0047 | Val Loss: 0.0241\n",
      "Epoch 126 | Train Loss: 0.0044 | Val Loss: 0.0227\n",
      "Epoch 127 | Train Loss: 0.0049 | Val Loss: 0.0240\n",
      "Epoch 128 | Train Loss: 0.0043 | Val Loss: 0.0231\n",
      "Epoch 129 | Train Loss: 0.0042 | Val Loss: 0.0277\n",
      "Epoch 130 | Train Loss: 0.0043 | Val Loss: 0.0251\n",
      "Epoch 131 | Train Loss: 0.0050 | Val Loss: 0.0294\n",
      "Epoch 132 | Train Loss: 0.0048 | Val Loss: 0.0218\n",
      "Epoch 133 | Train Loss: 0.0042 | Val Loss: 0.0272\n",
      "Epoch 134 | Train Loss: 0.0041 | Val Loss: 0.0251\n",
      "Epoch 135 | Train Loss: 0.0042 | Val Loss: 0.0250\n",
      "Epoch 136 | Train Loss: 0.0042 | Val Loss: 0.0237\n",
      "Epoch 137 | Train Loss: 0.0037 | Val Loss: 0.0259\n",
      "Epoch 138 | Train Loss: 0.0036 | Val Loss: 0.0228\n",
      "Epoch 139 | Train Loss: 0.0035 | Val Loss: 0.0272\n",
      "Epoch 140 | Train Loss: 0.0038 | Val Loss: 0.0240\n",
      "Epoch 141 | Train Loss: 0.0034 | Val Loss: 0.0253\n",
      "Epoch 142 | Train Loss: 0.0032 | Val Loss: 0.0240\n",
      "Epoch 143 | Train Loss: 0.0032 | Val Loss: 0.0251\n",
      "Epoch 144 | Train Loss: 0.0029 | Val Loss: 0.0244\n",
      "Epoch 145 | Train Loss: 0.0029 | Val Loss: 0.0257\n",
      "Epoch 146 | Train Loss: 0.0030 | Val Loss: 0.0223\n",
      "Epoch 147 | Train Loss: 0.0033 | Val Loss: 0.0249\n",
      "Epoch 148 | Train Loss: 0.0030 | Val Loss: 0.0260\n",
      "Epoch 149 | Train Loss: 0.0030 | Val Loss: 0.0243\n",
      "Epoch 150 | Train Loss: 0.0027 | Val Loss: 0.0257\n",
      "Epoch 151 | Train Loss: 0.0026 | Val Loss: 0.0232\n",
      "Epoch 152 | Train Loss: 0.0026 | Val Loss: 0.0251\n",
      "Epoch 153 | Train Loss: 0.0026 | Val Loss: 0.0257\n",
      "Epoch 154 | Train Loss: 0.0030 | Val Loss: 0.0229\n",
      "Epoch 155 | Train Loss: 0.0033 | Val Loss: 0.0271\n",
      "Epoch 156 | Train Loss: 0.0031 | Val Loss: 0.0246\n",
      "Epoch 157 | Train Loss: 0.0026 | Val Loss: 0.0256\n",
      "Epoch 158 | Train Loss: 0.0024 | Val Loss: 0.0264\n",
      "Epoch 159 | Train Loss: 0.0023 | Val Loss: 0.0243\n",
      "Epoch 160 | Train Loss: 0.0023 | Val Loss: 0.0255\n",
      "Epoch 161 | Train Loss: 0.0024 | Val Loss: 0.0251\n",
      "Epoch 162 | Train Loss: 0.0026 | Val Loss: 0.0267\n",
      "Epoch 163 | Train Loss: 0.0026 | Val Loss: 0.0264\n",
      "Epoch 164 | Train Loss: 0.0025 | Val Loss: 0.0268\n",
      "Epoch 165 | Train Loss: 0.0030 | Val Loss: 0.0256\n",
      "Epoch 166 | Train Loss: 0.0029 | Val Loss: 0.0264\n",
      "Epoch 167 | Train Loss: 0.0025 | Val Loss: 0.0262\n",
      "Epoch 168 | Train Loss: 0.0023 | Val Loss: 0.0264\n",
      "Epoch 169 | Train Loss: 0.0021 | Val Loss: 0.0258\n",
      "Epoch 170 | Train Loss: 0.0020 | Val Loss: 0.0279\n",
      "Epoch 171 | Train Loss: 0.0024 | Val Loss: 0.0262\n",
      "Epoch 172 | Train Loss: 0.0030 | Val Loss: 0.0263\n",
      "Epoch 173 | Train Loss: 0.0027 | Val Loss: 0.0241\n",
      "Epoch 174 | Train Loss: 0.0028 | Val Loss: 0.0266\n",
      "Epoch 175 | Train Loss: 0.0021 | Val Loss: 0.0264\n",
      "Epoch 176 | Train Loss: 0.0020 | Val Loss: 0.0261\n",
      "Epoch 177 | Train Loss: 0.0018 | Val Loss: 0.0275\n",
      "Epoch 178 | Train Loss: 0.0017 | Val Loss: 0.0282\n",
      "Epoch 179 | Train Loss: 0.0016 | Val Loss: 0.0254\n",
      "Epoch 180 | Train Loss: 0.0016 | Val Loss: 0.0259\n",
      "Epoch 181 | Train Loss: 0.0016 | Val Loss: 0.0283\n",
      "Epoch 182 | Train Loss: 0.0014 | Val Loss: 0.0263\n",
      "Epoch 183 | Train Loss: 0.0013 | Val Loss: 0.0275\n",
      "Epoch 184 | Train Loss: 0.0013 | Val Loss: 0.0268\n",
      "Epoch 185 | Train Loss: 0.0012 | Val Loss: 0.0282\n",
      "Epoch 186 | Train Loss: 0.0014 | Val Loss: 0.0277\n",
      "Epoch 187 | Train Loss: 0.0015 | Val Loss: 0.0272\n",
      "Epoch 188 | Train Loss: 0.0015 | Val Loss: 0.0293\n",
      "Epoch 189 | Train Loss: 0.0014 | Val Loss: 0.0278\n",
      "Epoch 190 | Train Loss: 0.0013 | Val Loss: 0.0280\n",
      "Epoch 191 | Train Loss: 0.0012 | Val Loss: 0.0279\n",
      "Epoch 192 | Train Loss: 0.0012 | Val Loss: 0.0271\n",
      "Epoch 193 | Train Loss: 0.0013 | Val Loss: 0.0277\n",
      "Epoch 194 | Train Loss: 0.0015 | Val Loss: 0.0298\n",
      "Epoch 195 | Train Loss: 0.0014 | Val Loss: 0.0276\n",
      "Epoch 196 | Train Loss: 0.0012 | Val Loss: 0.0274\n",
      "Epoch 197 | Train Loss: 0.0013 | Val Loss: 0.0305\n",
      "Epoch 198 | Train Loss: 0.0012 | Val Loss: 0.0277\n",
      "Epoch 199 | Train Loss: 0.0011 | Val Loss: 0.0308\n",
      "Epoch 200 | Train Loss: 0.0011 | Val Loss: 0.0275\n"
     ]
    }
   ],
   "source": [
    "# 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "\n",
    "    # (1) 데이터 로딩—예: NumPy array of shape [T, C]\n",
    "    data = numeric_df                             # 내 데이터\n",
    "    # 2) 정규화\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    \n",
    "    seq_len, pred_len = 4096, 24\n",
    "    ds = SlidingWindowDataset(data, seq_len, pred_len, stride=24)\n",
    "    train_size = int(len(ds)*0.8)\n",
    "    val_size   = len(ds) - train_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "    # (2) 모델/최적화/손실함수\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN_LSTM(in_channels=data.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # (3) 학습 루프\n",
    "    epochs = 200\n",
    "    for e in range(1, epochs+1):\n",
    "        tr_loss = train(model, train_loader, optim, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader,   criterion, device)\n",
    "        print(f\"Epoch {e:02d} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # (4) 예측\n",
    "    #x_sample, _ = ds[0]\n",
    "    #x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    #y_pred = model(x_sample)                     # [1, pred_len]\n",
    "    #print(\"Next 24h forecast:\", y_pred.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c292b4e-79d7-420a-9727-dd165e41c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 6) 샘플 예측\n",
    "    x_sample, y_true = ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_scaled = y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd90f4ba-d94a-4f65-8af9-5b54f2d1826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True next-24h (원단위): [ 6.0787883   3.021766    1.319169    0.14787701  0.05296     0.05288\n",
      "  0.0448      0.          0.          0.          0.          0.\n",
      "  0.          0.063022    1.079396    2.910719    5.464043    8.924349\n",
      " 10.057772   10.173965    9.227369   10.90645    10.762956    9.49266   ]\n",
      "Pred next-24h (원단위): [ 6.2275887   3.3489795   1.5331137   0.18458556  0.2249216   0.11592311\n",
      "  0.22410633  0.07984954 -0.01098471 -0.0453523  -0.05625914 -0.11521157\n",
      " -0.01218295  0.14496407  0.9476556   2.8665648   5.8001394   8.711964\n",
      "  9.398187    9.538769    9.32058    10.021412   10.532715    9.200582  ]\n"
     ]
    }
   ],
   "source": [
    "# --- 역스케일링 시작 ---\n",
    "import numpy as np\n",
    "solar_idx = 0\n",
    "\n",
    "# 전체 피처 개수\n",
    "C = data.shape[1]\n",
    "\n",
    "# 예측/실제 배열을 (pred_len, C) 모양으로 만들고 \n",
    "pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "\n",
    "# solar generation 채널(solar_idx)에만 값 채우기\n",
    "pred_full[:, solar_idx] = y_pred_scaled\n",
    "true_full[:, solar_idx] = y_true_scaled\n",
    "\n",
    "# MinMaxScaler.inverse_transform\n",
    "pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "# --- 역스케일링 끝 ---\n",
    "\n",
    "print(\"True next-24h (원단위):\", true_orig)\n",
    "print(\"Pred next-24h (원단위):\", pred_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44139c42-648d-439a-bf74-23acfcbfa021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3041\n",
      "MAPE: 6001800721203200.00%\n",
      "R2: 0.9950\n"
     ]
    }
   ],
   "source": [
    "# --- RMSE 계산 ---\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(np.mean((pred_orig - true_orig) ** 2))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fc82d-65b8-4052-b68a-f49de0731d2f",
   "metadata": {},
   "source": [
    "## 보름치 데이터\n",
    "보름치 데이터로 하루 예측한 RMSE와\n",
    "디퓨전 생성모형을 통해 보름치 데이터를 여러개 만들어 넣은 걸 합친 데이터로 하루 예측한 RMSE로 \n",
    "\n",
    "두 가지 경우를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54e90659-eac1-4fec-b1c8-a1bd8ad6cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv('merged_data_processed_seoul.csv', low_memory=False)\n",
    "\n",
    "seq_len  = 720   # 학습용: 보름치 (1h 단위 → 360h)\n",
    "pred_len =  24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df_short = df[-(seq_len + pred_len) : ]  # \n",
    "numeric_df_short = df_short.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df_short = numeric_df_short.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882ea1d4-84c8-4306-a055-fe0c4e15a732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/100] RMSE = 0.4662\n",
      "[002/100] RMSE = 0.7495\n",
      "[003/100] RMSE = 0.7263\n",
      "[004/100] RMSE = 0.6411\n",
      "[005/100] RMSE = 1.5999\n",
      "[006/100] RMSE = 0.5787\n",
      "[007/100] RMSE = 0.6348\n",
      "[008/100] RMSE = 0.7705\n",
      "[009/100] RMSE = 0.6743\n",
      "[010/100] RMSE = 0.5403\n",
      "[011/100] RMSE = 0.7780\n",
      "[012/100] RMSE = 0.6146\n",
      "[013/100] RMSE = 0.8313\n",
      "[014/100] RMSE = 0.6395\n",
      "[015/100] RMSE = 0.6787\n",
      "[016/100] RMSE = 0.5581\n",
      "[017/100] RMSE = 0.5352\n",
      "[018/100] RMSE = 0.4561\n",
      "[019/100] RMSE = 0.8092\n",
      "[020/100] RMSE = 1.0141\n",
      "[021/100] RMSE = 0.5592\n",
      "[022/100] RMSE = 0.7798\n",
      "[023/100] RMSE = 0.6662\n",
      "[024/100] RMSE = 0.6364\n",
      "[025/100] RMSE = 0.5048\n",
      "[026/100] RMSE = 0.4885\n",
      "[027/100] RMSE = 0.8455\n",
      "[028/100] RMSE = 0.4486\n",
      "[029/100] RMSE = 0.6206\n",
      "[030/100] RMSE = 0.5901\n",
      "[031/100] RMSE = 0.4101\n",
      "[032/100] RMSE = 0.5626\n",
      "[033/100] RMSE = 0.6233\n",
      "[034/100] RMSE = 0.4702\n",
      "[035/100] RMSE = 1.1726\n",
      "[036/100] RMSE = 0.6664\n",
      "[037/100] RMSE = 0.6852\n",
      "[038/100] RMSE = 0.9758\n",
      "[039/100] RMSE = 0.5120\n",
      "[040/100] RMSE = 0.6660\n",
      "[041/100] RMSE = 0.5082\n",
      "[042/100] RMSE = 0.5738\n",
      "[043/100] RMSE = 0.5539\n",
      "[044/100] RMSE = 0.8451\n",
      "[045/100] RMSE = 2.4029\n",
      "[046/100] RMSE = 0.6721\n",
      "[047/100] RMSE = 0.5746\n",
      "[048/100] RMSE = 0.8267\n",
      "[049/100] RMSE = 0.4363\n",
      "[050/100] RMSE = 0.7956\n",
      "[051/100] RMSE = 0.6192\n",
      "[052/100] RMSE = 0.5561\n",
      "[053/100] RMSE = 0.7454\n",
      "[054/100] RMSE = 0.3612\n",
      "[055/100] RMSE = 0.6297\n",
      "[056/100] RMSE = 0.6430\n",
      "[057/100] RMSE = 1.2405\n",
      "[058/100] RMSE = 0.8284\n",
      "[059/100] RMSE = 0.6082\n",
      "[060/100] RMSE = 0.5999\n",
      "[061/100] RMSE = 0.9681\n",
      "[062/100] RMSE = 0.4735\n",
      "[063/100] RMSE = 1.2089\n",
      "[064/100] RMSE = 0.4751\n",
      "[065/100] RMSE = 0.5564\n",
      "[066/100] RMSE = 0.9813\n",
      "[067/100] RMSE = 0.4836\n",
      "[068/100] RMSE = 0.8533\n",
      "[069/100] RMSE = 0.7329\n",
      "[070/100] RMSE = 0.4822\n",
      "[071/100] RMSE = 0.4458\n",
      "[072/100] RMSE = 0.5422\n",
      "[073/100] RMSE = 0.6431\n",
      "[074/100] RMSE = 2.2687\n",
      "[075/100] RMSE = 0.5205\n",
      "[076/100] RMSE = 0.7813\n",
      "[077/100] RMSE = 0.3996\n",
      "[078/100] RMSE = 0.5458\n",
      "[079/100] RMSE = 0.9869\n",
      "[080/100] RMSE = 0.6438\n",
      "[081/100] RMSE = 0.6276\n",
      "[082/100] RMSE = 0.5517\n",
      "[083/100] RMSE = 0.6656\n",
      "[084/100] RMSE = 0.6810\n",
      "[085/100] RMSE = 0.4889\n",
      "[086/100] RMSE = 0.6112\n",
      "[087/100] RMSE = 0.6829\n",
      "[088/100] RMSE = 1.0990\n",
      "[089/100] RMSE = 0.5779\n",
      "[090/100] RMSE = 0.8492\n",
      "[091/100] RMSE = 0.4311\n",
      "[092/100] RMSE = 0.5306\n",
      "[093/100] RMSE = 0.5286\n",
      "[094/100] RMSE = 1.4791\n",
      "[095/100] RMSE = 0.8987\n",
      "[096/100] RMSE = 0.5143\n",
      "[097/100] RMSE = 0.6160\n",
      "[098/100] RMSE = 0.7684\n",
      "[099/100] RMSE = 0.4724\n",
      "[100/100] RMSE = 0.7584\n",
      "\n",
      "========== 100-run Summary ==========\n",
      "Mean RMSE  : 0.7103\n",
      "Variance   : 0.100638\n",
      "Std. Dev.  : 0.3172\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, random, gc\n",
    "\n",
    "NUM_RUNS = 100\n",
    "RMSEs    = []\n",
    "\n",
    "for run in range(1, NUM_RUNS+1):\n",
    "\n",
    "    # ── 1) 랜덤시드 고정 (run 값으로 변주) ────────────\n",
    "    seed = 42 + run\n",
    "    torch.manual_seed(seed);   np.random.seed(seed);   random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        import numpy as np\n",
    "\n",
    "        #  데이터 로딩—예: NumPy array of shape [T, C]\n",
    "        data = numeric_df_short                             # 내 데이터\n",
    "        #  정규화\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "    \n",
    "        seq_len, pred_len = 24, 24\n",
    "        ds = SlidingWindowDataset(data, seq_len, pred_len, stride=24)\n",
    "        train_size = int(len(ds)*0.8)\n",
    "        val_size   = len(ds) - train_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "        train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "        # 모델/최적화/손실함수\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = CNN_LSTM(in_channels=data.shape[1]).to(device)\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # 학습 루프\n",
    "        epochs = 100\n",
    "        for e in range(1, epochs+1):\n",
    "            tr_loss = train(model, train_loader, optim, criterion, device)\n",
    "            val_loss = evaluate(model, val_loader,   criterion, device)\n",
    "            #print(f\"Epoch {e:02d} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "        # 샘플 예측\n",
    "        x_sample, y_true = ds[0]\n",
    "        x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "        y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "        y_true_scaled = y_true.numpy()\n",
    "\n",
    "    # --- 역스케일링 시작 ---\n",
    "    solar_idx = 0\n",
    "\n",
    "    # 전체 피처 개수\n",
    "    C = data.shape[1]\n",
    "\n",
    "    # 예측/실제 배열을 (pred_len, C) 모양으로 만들고 \n",
    "    pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "\n",
    "    # solar generation 채널(solar_idx)에만 값 채우기\n",
    "    pred_full[:, solar_idx] = y_pred_scaled\n",
    "    true_full[:, solar_idx] = y_true_scaled\n",
    "\n",
    "    # MinMaxScaler.inverse_transform\n",
    "    pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "    true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "    # --- 역스케일링 끝 ---\n",
    "\n",
    "    # --- RMSE 계산 ---\n",
    "    import numpy as np\n",
    "\n",
    "    rmse = np.sqrt(np.mean((pred_orig - true_orig) ** 2))\n",
    "    #print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "    RMSEs.append(rmse)       # 결과 저장\n",
    "    print(f\"[{run:03d}/{NUM_RUNS}] RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    # ── 3) GPU 메모리·파이썬 객체 청소 (필수는 아니지만 안전) ──\n",
    "    del model, train_loader, val_loader\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# ── 4) 통계량 출력 ─────────────────────────────────────\n",
    "RMSEs = np.array(RMSEs)\n",
    "mean  = RMSEs.mean()\n",
    "var   = RMSEs.var(ddof=0)          # 불편분산은 ddof=1\n",
    "std   = RMSEs.std(ddof=0)\n",
    "\n",
    "print(\"\\n========== 100-run Summary ==========\")\n",
    "print(f\"Mean RMSE  : {mean:.4f}\")\n",
    "print(f\"Variance   : {var:.6f}\")\n",
    "print(f\"Std. Dev.  : {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81f50a00-839a-40d6-8e16-43c55b84d452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(inf)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8d038-0e53-4158-81da-61039897d08c",
   "metadata": {},
   "source": [
    "## 보름치 데이터\n",
    "보름치 데이터로 하루 예측한 RMSE와\n",
    "\n",
    "디퓨전 생성모형을 통해 보름치 데이터를 여러개 만들어 넣은 걸 합친 데이터로 하루 예측한 RMSE로 \n",
    "\n",
    "두 가지 경우를 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299dc3e-f77c-44e9-b29b-105b55f3d65b",
   "metadata": {},
   "source": [
    "### 보름치 데이터로 하루 예측한 RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419743e-da1f-418d-9c48-9a3a459fd2da",
   "metadata": {},
   "source": [
    "### 디퓨전 생성모형을 통해 보름치 데이터를 여러개 만들어 넣은 걸 합친 데이터로 하루 예측한 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e920f5b9-2a99-4358-bdad-95e08eca7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv('merged_data_processed_seoul.csv', low_memory=False)\n",
    "\n",
    "seq_len  = 720   # 학습용: 보름치 (1h 단위 → 360h)\n",
    "pred_len =  24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df_short = df[-(seq_len + pred_len) : ]  # \n",
    "numeric_df_short = df_short.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df_short = numeric_df_short.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bf284b0-912a-43ab-8b98-432468748888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 'array.pkl'에 저장된 NumPy 배열을 불러오기\n",
    "with open('/home1/gkrtod35/Diffusion-TS/merged_results/merged_seq720.pkl', 'rb') as f:\n",
    "    merged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6d517ae-2e6d-4a77-848f-9f88f09fece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성 데이터에서 X_synth, Y_synth 뽑기\n",
    "X_synth = merged[:, :seq_len, :]            # (N_synth, seq_len, C)\n",
    "Y_synth = merged[:, seq_len:seq_len+pred_len, 0]  # (N_synth, pred_len)  # 채널0=타깃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72184347-ca99-498d-a6ef-521c1c8995af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  데이터 로딩—예: NumPy array of shape [T, C]\n",
    "data = numeric_df_short                             # 내 데이터\n",
    "        #  정규화\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "    \n",
    "seq_len, pred_len = 24, 24\n",
    "\n",
    "# data.shape == (T, C)\n",
    "ds_real = SlidingWindowDataset(data, seq_len=seq_len, pred_len=pred_len, stride=24)\n",
    "# ds_real.X.shape == (N_real, seq_len, C)\n",
    "# ds_real.Y.shape == (N_real, pred_len)\n",
    "\n",
    "X_real = ds_real.X.numpy()\n",
    "Y_real = ds_real.Y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f694ed56-2ec8-4f6c-aab7-45b03f4334d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 48, 19)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "978571a5-0241-4eed-bc48-c3f5e1a5c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1) 합치기\n",
    "X_all = np.concatenate([X_real, X_synth], axis=0)  # (N_real+N_synth, seq_len, C)\n",
    "Y_all = np.concatenate([Y_real, Y_synth], axis=0)  # (N_real+N_synth, pred_len)\n",
    "\n",
    "# 2) 텐서로 변환하고, 채널 축을 앞쪽으로 옮기기: [N, C, seq_len]\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32).transpose(1,2)\n",
    "Y_all = torch.tensor(Y_all, dtype=torch.float32)\n",
    "\n",
    "# 3) Dataset & DataLoader\n",
    "combined_ds = TensorDataset(X_all, Y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fbea4-6bde-47a9-b6ef-b1cde7afb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 0) 필요 모듈\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import torch, numpy as np, random, gc\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.auto import tqdm                    # 진행-bar 보기 좋게\n",
    "# 이미 선언돼 있는 클래스·함수·데이터:\n",
    "#   combined_ds, data, scaler, pred_len\n",
    "#   CNN_LSTM_Transformer, train_epoch, eval_epoch\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 단일 실험(학습→RMSE) 함수\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def run_once(run_idx: int, epochs: int = 100, batch: int = 8) -> float:\n",
    "    \"\"\"학습 1회 수행 후 RMSE 반환\"\"\"\n",
    "    # 1-a. 시드 고정 (run_idx 로 변주)\n",
    "    seed = 42 + run_idx\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # 1-b. train/val 재분할\n",
    "    n_train = int(len(combined_ds) * 0.8)\n",
    "    n_val   = len(combined_ds) - n_train\n",
    "    train_ds, val_ds = random_split(combined_ds, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch)\n",
    "\n",
    "    # 1-c. 모델·옵티마이저·손실\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = CNN_LSTM_Transformer(in_channels=data.shape[1]).to(device)\n",
    "    optim  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    crit   = torch.nn.MSELoss()\n",
    "\n",
    "    # 1-d. 학습 루프\n",
    "    for _ in range(epochs):\n",
    "        train_epoch(model, train_loader, optim, crit, device)\n",
    "        # val_loss = eval_epoch(model, val_loader, crit, device)  # 선택\n",
    "\n",
    "    # 1-e. 샘플 예측 (첫 윈도우)\n",
    "    x_sample, y_true = combined_ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)\n",
    "    y_pred_s = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_s = y_true.numpy()\n",
    "\n",
    "    # 1-f. 역스케일링 & RMSE\n",
    "    solar_idx = 0\n",
    "    C = data.shape[1]\n",
    "    pf, tf = np.zeros((pred_len, C), np.float32), np.zeros((pred_len, C), np.float32)\n",
    "    pf[:, solar_idx] = y_pred_s; tf[:, solar_idx] = y_true_s\n",
    "    pred = scaler.inverse_transform(pf)[:, solar_idx]\n",
    "    true = scaler.inverse_transform(tf)[:, solar_idx]\n",
    "    rmse = np.sqrt(np.mean((pred - true) ** 2))\n",
    "\n",
    "    # 1-g. 메모리 정리\n",
    "    del model, train_loader, val_loader; torch.cuda.empty_cache(); gc.collect()\n",
    "    return rmse\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 100-run 반복\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "NUM_RUNS = 10\n",
    "rmse_list = []\n",
    "\n",
    "for i in tqdm(range(NUM_RUNS), desc=\"100 runs\"):\n",
    "    rmse_val = run_once(i+1)\n",
    "    rmse_list.append(rmse_val)\n",
    "\n",
    "rmse_arr = np.array(rmse_list)\n",
    "mean_rmse = rmse_arr.mean()\n",
    "std_rmse  = rmse_arr.std(ddof=0)      # 표본표준편차는 ddof=1\n",
    "\n",
    "print(\"\\n========== 100-run Summary ==========\")\n",
    "print(f\"Mean RMSE : {mean_rmse:.4f}\")\n",
    "print(f\"Std  RMSE : {std_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "372b5704-e21a-48a9-a5ca-70eaf732f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 7.4492 | Val Loss: 4.8954\n",
      "Epoch 02 | Train Loss: 3.2717 | Val Loss: 2.8391\n",
      "Epoch 03 | Train Loss: 2.6419 | Val Loss: 2.6577\n",
      "Epoch 04 | Train Loss: 2.5741 | Val Loss: 2.5186\n",
      "Epoch 05 | Train Loss: 2.2193 | Val Loss: 2.0384\n",
      "Epoch 06 | Train Loss: 1.4519 | Val Loss: 0.8760\n",
      "Epoch 07 | Train Loss: 0.6861 | Val Loss: 0.4759\n",
      "Epoch 08 | Train Loss: 0.3145 | Val Loss: 0.3119\n",
      "Epoch 09 | Train Loss: 0.1906 | Val Loss: 0.2356\n",
      "Epoch 10 | Train Loss: 0.1447 | Val Loss: 0.2216\n",
      "Epoch 11 | Train Loss: 0.1232 | Val Loss: 0.2195\n",
      "Epoch 12 | Train Loss: 0.1204 | Val Loss: 0.2121\n",
      "Epoch 13 | Train Loss: 0.1137 | Val Loss: 0.2135\n",
      "Epoch 14 | Train Loss: 0.1123 | Val Loss: 0.2118\n",
      "Epoch 15 | Train Loss: 0.1120 | Val Loss: 0.2139\n",
      "Epoch 16 | Train Loss: 0.1101 | Val Loss: 0.2136\n",
      "Epoch 17 | Train Loss: 0.1090 | Val Loss: 0.2116\n",
      "Epoch 18 | Train Loss: 0.1108 | Val Loss: 0.2107\n",
      "Epoch 19 | Train Loss: 0.1101 | Val Loss: 0.2119\n",
      "Epoch 20 | Train Loss: 0.1090 | Val Loss: 0.2125\n",
      "Epoch 21 | Train Loss: 0.1106 | Val Loss: 0.2202\n",
      "Epoch 22 | Train Loss: 0.1136 | Val Loss: 0.2118\n",
      "Epoch 23 | Train Loss: 0.1072 | Val Loss: 0.2145\n",
      "Epoch 24 | Train Loss: 0.1082 | Val Loss: 0.2127\n",
      "Epoch 25 | Train Loss: 0.1065 | Val Loss: 0.2161\n",
      "Epoch 26 | Train Loss: 0.1126 | Val Loss: 0.2122\n",
      "Epoch 27 | Train Loss: 0.1098 | Val Loss: 0.2119\n",
      "Epoch 28 | Train Loss: 0.1085 | Val Loss: 0.2167\n",
      "Epoch 29 | Train Loss: 0.1095 | Val Loss: 0.2107\n",
      "Epoch 30 | Train Loss: 0.1063 | Val Loss: 0.2125\n",
      "RMSE: 1.9725\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 5) train/val \n",
    "if __name__ == \"__main__\":\n",
    "    n_train = int(len(combined_ds) * 0.8)\n",
    "    n_val   = len(combined_ds) - n_train\n",
    "    train_ds, val_ds = random_split(combined_ds, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "    # 6) 모델·옵티마이저·손실함수 세팅\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN_LSTM(in_channels=data.shape[1]).to(device)\n",
    "    optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 학습 루프\n",
    "    epochs = 30\n",
    "    for e in range(1, epochs+1):\n",
    "        tr_loss = train(model, train_loader, optim, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader,   criterion, device)\n",
    "        print(f\"Epoch {e:02d} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 8) 샘플 예측 + 역스케일링 + RMSE 계산\n",
    "    x_sample, y_true = combined_ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_scaled = y_true.numpy()\n",
    "\n",
    "    # 역스케일링\n",
    "    solar_idx = 0\n",
    "    C = data.shape[1]\n",
    "    pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    pred_full[:, solar_idx] = y_pred_scaled\n",
    "    true_full[:, solar_idx] = y_true_scaled\n",
    "    pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "    true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "\n",
    "    rmse = np.sqrt(np.mean((pred_orig - true_orig)**2))\n",
    "    print(f\"RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
