{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd6ab37-b4aa-4cd0-bc59-ce0aa2ecceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfs/home1/gkrtod35/Diffusion-TS'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/gpfs/home1/gkrtod35/Diffusion-TS')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedd1a3e-f859-4940-8d38-3ffb234ce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from engine.solver import Trainer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Utils.io_utils import load_yaml_config, instantiate_from_config\n",
    "from Models.interpretable_diffusion.model_utils import normalize_to_neg_one_to_one, unnormalize_to_zero_to_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c223033-d77e-4cd7-8251-10fcbf68c245",
   "metadata": {},
   "source": [
    "## Orginal\n",
    "\n",
    "전체 구간의 길이를 학습해서 만든 것이 아닌 짧게 보름치 데이터만 학습시켜 데이터를 생성한후 예측모형에 넣기 위해 진행해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664d0eac-055f-47c7-8e5b-80caccb57f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home1/gkrtod35/Diffusion-TS/Data/merged_data_processed_seoul.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dd68ad-753f-435d-9002-4b5558292811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15일치 데이터로 자르기\n",
    "seq_len  = 360   # 학습용: 보름치 (1h 단위 → 15*24=360h)\n",
    "pred_len =  24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df_short = df[-(seq_len + pred_len) : ]  # shape (8760, C)\n",
    "\n",
    "numeric_df_short = df_short.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df_short = numeric_df_short.apply(pd.to_numeric, errors='coerce')\n",
    "numeric_df_short = numeric_df_short.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff383d7-d076-41b0-97dd-6ecfbf8e2ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.177255: 100%|██████████| 1000/1000 [01:31<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "=== Run 1/100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.068326: 100%|██████████| 1000/1000 [01:34<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [00:12<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run 2/100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.065508:   2%|▏         | 22/1000 [00:02<01:33, 10.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# 1) 학습 (config에 설정된 에포크만큼)\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 2) 복원\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#    shape=[seq_len, feat_num] 은 마스킹 전 입력 시퀀스 길이입니다.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m sample_i, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mrestore(\n\u001b[1;32m     87\u001b[0m     test_loader,\n\u001b[1;32m     88\u001b[0m     shape\u001b[38;5;241m=\u001b[39m[win_size, feat_num],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     sampling_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m/gpfs/home1/gkrtod35/Diffusion-TS/engine/solver.py:111\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data, target\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    110\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    114\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─── 하이퍼파라미터 ───\n",
    "seq_len  = 24    # 예측모형 입력 길이\n",
    "pred_len = 24    # 예측모형 예측 길이\n",
    "stride   = 24    # 슬라이딩 윈도우 이동 간격\n",
    "win_size = seq_len + pred_len  # = 48\n",
    "n_runs   = 5     # 복원 반복 횟수\n",
    "feat_num = 19\n",
    "\n",
    "# ─── 1) 원본 시계열을 윈도우(48) 단위로 잘라서 seqs 생성 ───\n",
    "# data_array: 정규화(스케일링) 이전의 원본 NumPy array, shape=(T, C)\n",
    "seqs = []\n",
    "for i in range(0, len(numeric_df_short) - win_size + 1, stride):\n",
    "    seqs.append(numeric_df_short[i : i + win_size, :])   # [48, C]\n",
    "seqs = np.stack(seqs, axis=0)                       # [N_windows, 48, C]\n",
    "\n",
    "# ─── 2) train/test 분할 & 스케일링(기존 그대로) ───\n",
    "n_train      = int(len(seqs) * 0.8)\n",
    "train, test  = seqs[:n_train], seqs[n_train:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# train: (n_train*72, D) → fit → reshape back\n",
    "train_scaled = scaler.fit_transform(train.reshape(-1, train.shape[-1]))\n",
    "train_scaled = normalize_to_neg_one_to_one(train_scaled).reshape(train.shape)\n",
    "# test: 같은 방식으로 transform → reshape → normalize\n",
    "test_scaled = scaler.transform(test.reshape(-1, test.shape[-1]))\n",
    "test_scaled = normalize_to_neg_one_to_one(test_scaled).reshape(test.shape)\n",
    "\n",
    "# ─── 3) SeqDataset 생성 (pred_length=24) ───\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, pred_length=24, regular=True):\n",
    "        self.data    = data\n",
    "        self.regular = regular\n",
    "        # 마지막 24타임스텝만 False로 마스킹\n",
    "        self.mask    = np.ones_like(data, dtype=bool)\n",
    "        self.mask[:, -pred_length:, :] = False  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx]).float()\n",
    "        if self.regular:\n",
    "            return x\n",
    "        m = torch.from_numpy(self.mask[idx]).bool()\n",
    "        return x, m\n",
    "\n",
    "train_ds = SeqDataset(train_scaled, pred_length=pred_len, regular=True)\n",
    "test_ds  = SeqDataset(test_scaled,  pred_length=pred_len, regular=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=len(test_ds), shuffle=False)\n",
    "\n",
    "\n",
    "# 모델 설정 및 학습\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config_path = './Config/solar.yaml'\n",
    "        self.save_dir    = './forecasting_exp'\n",
    "        self.gpu         = 0\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "args   = Args()\n",
    "configs = load_yaml_config(args.config_path)\n",
    "device  = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model   = instantiate_from_config(configs['model']).to(device)\n",
    "trainer = Trainer(\n",
    "    config=configs,\n",
    "    args=args,\n",
    "    model=model,\n",
    "    dataloader={'dataloader': train_loader}\n",
    ")\n",
    "trainer.train()   # 이제 next(self.dl) 은 Tensor 하나만 반환합니다\n",
    "\n",
    "# 반복 학습 + 복원 그리고 합치기\n",
    "n_runs = 100\n",
    "all_samples = []  # 여기에 매번 복원된 결과를 붙여나갑니다\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"=== Run {run+1}/{n_runs} ===\")\n",
    "    \n",
    "    # 1) 학습 (config에 설정된 에포크만큼)\n",
    "    trainer.train()\n",
    "    \n",
    "    # 2) 복원\n",
    "    #    shape=[seq_len, feat_num] 은 마스킹 전 입력 시퀀스 길이입니다.\n",
    "    sample_i, *_ = trainer.restore(\n",
    "        test_loader,\n",
    "        shape=[win_size, feat_num],\n",
    "        coef=1e-2,\n",
    "        stepsize=5e-2,\n",
    "        sampling_steps=200\n",
    "    )\n",
    "    \n",
    "    # 3) 역스케일 & 원래 형태로 복원\n",
    "    sample_i = scaler.inverse_transform(\n",
    "                   unnormalize_to_zero_to_one(sample_i.reshape(-1, feat_num))\n",
    "               ).reshape(test_scaled.shape)  # (batch, seq_len, feat_num)\n",
    "    \n",
    "    all_samples.append(sample_i)\n",
    "\n",
    "# 4) 하나의 큰 배열로 합치기\n",
    "#    all_samples 는 길이 5짜리 리스트, 각 원소는 (batch, seq_len, feat_num) 모양\n",
    "#    concatenate 하면 (5*batch, seq_len, feat_num) 크기로 합쳐집니다\n",
    "merged = np.concatenate(all_samples, axis=0)\n",
    "\n",
    "# merged.shape 출력해 보기\n",
    "print(\"merged shape:\", merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c7b74e-f4ae-41af-b2cb-87aa961b3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('array.pkl', 'wb') as f:\n",
    "    pickle.dump(merged, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b074754-a98f-491e-b837-20c1bd771e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 48, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729cae3c-3a5e-49cb-bf32-dbd65be7042c",
   "metadata": {},
   "source": [
    "## 시계열 길이\n",
    "\n",
    "첫번째 셀을 실행시키고 2,3,4번째 셀 중 하나를 실행시키면 된다\n",
    "\n",
    "데이터 학습의 길이를 30일,90일,6개월,1년 치로 자른 후 원하는 만큼을 생성 시켰습니다\n",
    "\n",
    "병렬처리가 가능한지 보려고 run_pipeline, single_run을 만들었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4674c4d0-1afe-497f-ac73-0d5ea09b5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, pred_length=24, regular=True):\n",
    "        self.data    = data\n",
    "        self.regular = regular\n",
    "        # 마지막 24타임스텝만 False로 마스킹\n",
    "        self.mask    = np.ones_like(data, dtype=bool)\n",
    "        self.mask[:, -pred_length:, :] = False  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx]).float()\n",
    "        if self.regular:\n",
    "            return x\n",
    "        m = torch.from_numpy(self.mask[idx]).bool()\n",
    "        return x, m\n",
    "\n",
    "# 모델 설정 및 학습\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.config_path = './Config/solar.yaml'\n",
    "        self.save_dir    = './forecasting_exp'\n",
    "        self.gpu         = 0\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "args   = Args()\n",
    "configs = load_yaml_config(args.config_path)\n",
    "device  = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model   = instantiate_from_config(configs['model']).to(device)\n",
    "trainer = Trainer(\n",
    "    config=configs,\n",
    "    args=args,\n",
    "    model=model,\n",
    "    dataloader={'dataloader': train_loader}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a656c73-2848-4144-b69c-10845abe4fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seq_len=720] run 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 54.981483:   4%|▍         | 42/1000 [00:21<08:14,  1.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m base_configs  \u001b[38;5;241m=\u001b[39m load_yaml_config(args\u001b[38;5;241m.\u001b[39mconfig_path)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_len \u001b[38;5;129;01min\u001b[39;00m seq_len_list:\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_df_short\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./merged_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 75\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(seq_len, pred_len, stride, n_runs, feat_num, numeric_df_short, base_configs, args, save_dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_runs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[seq_len=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 내부 max_epochs 만큼 수행\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     sample_i, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mrestore(\n\u001b[1;32m     77\u001b[0m         test_loader,\n\u001b[1;32m     78\u001b[0m         shape\u001b[38;5;241m=\u001b[39m[win_size, feat_num],\n\u001b[1;32m     79\u001b[0m         coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-2\u001b[39m, sampling_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     sample_i \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(\n\u001b[1;32m     82\u001b[0m         unnormalize_to_zero_to_one(sample_i\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, feat_num))\n\u001b[1;32m     83\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(test_scaled\u001b[38;5;241m.\u001b[39mshape)                    \u001b[38;5;66;03m# (batch, win_size, feat_num)\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/home1/gkrtod35/Diffusion-TS/engine/solver.py:111\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data, target\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    110\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    114\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 필요한 외부 정의 (이미 import·정의돼 있다고 가정)\n",
    "#  - normalize_to_neg_one_to_one, unnormalize_to_zero_to_one\n",
    "#  - SeqDataset\n",
    "#  - Trainer\n",
    "#  - load_yaml_config\n",
    "#  - instantiate_from_config\n",
    "#  - Args  (config_path·gpu 등을 포함)\n",
    "\n",
    "import os, copy, pickle, pandas as pd\n",
    "import numpy as np, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def run_pipeline(seq_len, pred_len, stride, n_runs, feat_num,\n",
    "                 numeric_df_short, base_configs, args, save_dir):\n",
    "    \"\"\"\n",
    "    seq_len : 학습용 입력 길이 (720, 2160, …)\n",
    "    pred_len: 예측 길이 (24)\n",
    "    stride  : 슬라이딩 윈도우 간격 (24 → 하루)\n",
    "    n_runs  : 복원 반복 횟수\n",
    "    feat_num: 변수 개수\n",
    "    numeric_df_short: 원본 시계열 ndarray\n",
    "    \"\"\"\n",
    "    # 1) 윈도우 분할\n",
    "    win_size = seq_len + pred_len\n",
    "    seqs = [numeric_df_short[i:i+win_size]\n",
    "            for i in range(0, len(numeric_df_short)-win_size+1, stride)]\n",
    "    seqs = np.stack(seqs, axis=0)                      # [N, win_size, feat_num]\n",
    "\n",
    "    # 2) 학습/테스트 분할 + [-1,1] 스케일\n",
    "    n_train = int(len(seqs)*0.8)\n",
    "    train, test = seqs[:n_train], seqs[n_train:]\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.fit_transform(train.reshape(-1, feat_num))\n",
    "    ).reshape(train.shape)\n",
    "    test_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.transform(test.reshape(-1, feat_num))\n",
    "    ).reshape(test.shape)\n",
    "\n",
    "    # 3) Dataset / DataLoader\n",
    "    train_ds = SeqDataset(train_scaled, pred_length=pred_len, regular=True)\n",
    "    test_ds  = SeqDataset(test_scaled,  pred_length=pred_len, regular=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=len(test_ds), shuffle=False)\n",
    "\n",
    "    # 4) YAML 복사 후 seq_length 갱신\n",
    "    configs = copy.deepcopy(base_configs)\n",
    "    for k in ('seq_length','seq_len','max_seq_len','max_len','context_length'):\n",
    "        if k in configs['model']['params']:\n",
    "            configs['model']['params'][k] = win_size\n",
    "    configs['model']['params']['feature_size'] = feat_num\n",
    "\n",
    "    # 5) 모델·트레이너 초기화\n",
    "    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "    model  = instantiate_from_config(configs['model']).to(device)\n",
    "\n",
    "    # (선택) PositionalEncoding 길이 강제 교체 ─ 경로가 다르면 print(model)로 확인\n",
    "    try:\n",
    "        from Models.interpretable_diffusion.model_utils import LearnablePositionalEncoding\n",
    "        d_model = configs['model']['params']['d_model']\n",
    "        if hasattr(model, 'model') and hasattr(model.model, 'pos_enc'):\n",
    "            model.model.pos_enc = LearnablePositionalEncoding(d_model, max_len=win_size).to(device)\n",
    "    except ImportError:\n",
    "        pass  # 모듈 경로가 다르면 생략 → seq_length 갱신만으로 충분할 수도 있음\n",
    "\n",
    "    trainer = Trainer(config=configs, args=args,\n",
    "                      model=model, dataloader={'dataloader': train_loader})\n",
    "\n",
    "    # 6) 학습·복원 반복\n",
    "    all_samples = []\n",
    "    for run in range(1, n_runs+1):\n",
    "        print(f\"[seq_len={seq_len}] run {run}/{n_runs}\")\n",
    "        trainer.train()  # 내부 max_epochs 만큼 수행\n",
    "        sample_i, *_ = trainer.restore(\n",
    "            test_loader,\n",
    "            shape=[win_size, feat_num],\n",
    "            coef=1e-2, stepsize=5e-2, sampling_steps=200\n",
    "        )\n",
    "        sample_i = scaler.inverse_transform(\n",
    "            unnormalize_to_zero_to_one(sample_i.reshape(-1, feat_num))\n",
    "        ).reshape(test_scaled.shape)                    # (batch, win_size, feat_num)\n",
    "        all_samples.append(sample_i)\n",
    "\n",
    "    # 7) 합치고 저장\n",
    "    merged = np.concatenate(all_samples, axis=0)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = os.path.join(save_dir, f\"merged_seq{seq_len}.pkl\")\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(merged, f)\n",
    "    print(f\"✔ seq_len={seq_len} 완료 → {out_path}  shape={merged.shape}\")\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV 로드 & 숫자 컬럼만 추출\n",
    "    df = pd.read_csv(\n",
    "        '/home1/gkrtod35/Diffusion-TS/Data/merged_data_processed_seoul.csv',\n",
    "        low_memory=False\n",
    "    )\n",
    "    df_short = df[-(8640+24):]   # 가장 최근 8640+24시간만 사용\n",
    "    numeric_df_short = (df_short\n",
    "                        .drop(columns=['Idx','date','time','일시'])\n",
    "                        .apply(pd.to_numeric, errors='coerce')\n",
    "                        .to_numpy())\n",
    "\n",
    "    seq_len_list = [720, 2160, 4320, 8640]   # 실험할 입력 길이\n",
    "    pred_len     = 24\n",
    "    stride       = 24\n",
    "    n_runs       = 100\n",
    "    feat_num     = numeric_df_short.shape[1]\n",
    "\n",
    "    args          = Args()\n",
    "    base_configs  = load_yaml_config(args.config_path)\n",
    "\n",
    "    for seq_len in seq_len_list:\n",
    "        run_pipeline(seq_len, pred_len, stride, n_runs, feat_num,\n",
    "                     numeric_df_short, base_configs, args,\n",
    "                     save_dir='./merged_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92665362-74d2-4efe-898a-263eef81e890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seq_len=720] run 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.805898:  75%|███████▍  | 747/1000 [06:18<02:08,  1.97it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m base_configs \u001b[38;5;241m=\u001b[39m load_yaml_config(args\u001b[38;5;241m.\u001b[39mconfig_path)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_len \u001b[38;5;129;01min\u001b[39;00m seq_len_list:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnumeric_df_short\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./merged_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(seq_len, pred_len, stride, n_runs, feat_num, numeric_df_short, base_configs, args, save_dir)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_runs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[seq_len=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     sample_i, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mrestore(\n\u001b[1;32m     66\u001b[0m         test_loader,\n\u001b[1;32m     67\u001b[0m         shape\u001b[38;5;241m=\u001b[39m[win_size, feat_num],\n\u001b[1;32m     68\u001b[0m         coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, stepsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-2\u001b[39m, sampling_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     sample_i \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(\n\u001b[1;32m     71\u001b[0m         unnormalize_to_zero_to_one(sample_i\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, feat_num))\n\u001b[1;32m     72\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(test_scaled\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/gpfs/home1/gkrtod35/Diffusion-TS/engine/solver.py:111\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data, target\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m    110\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    114\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/df/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 필수 외부 정의: normalize_to_neg_one_to_one, unnormalize_to_zero_to_one,\n",
    "# SeqDataset, Trainer, load_yaml_config, instantiate_from_config, Args\n",
    "import os, copy, pickle, pandas as pd, numpy as np, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def run_pipeline(seq_len, pred_len, stride, n_runs, feat_num,\n",
    "                 numeric_df_short, base_configs, args, save_dir):\n",
    "\n",
    "    win_size = seq_len + pred_len                      # 744 등\n",
    "\n",
    "    # 1) 윈도우 분할\n",
    "    seqs = [numeric_df_short[i:i+win_size]\n",
    "            for i in range(0, len(numeric_df_short)-win_size+1, stride)]\n",
    "    seqs = np.stack(seqs, axis=0)\n",
    "\n",
    "    # 2) 스케일링\n",
    "    n_train = int(len(seqs)*0.8)\n",
    "    train, test = seqs[:n_train], seqs[n_train:]\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.fit_transform(train.reshape(-1, feat_num))\n",
    "    ).reshape(train.shape)\n",
    "    test_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.transform(test.reshape(-1, feat_num))\n",
    "    ).reshape(test.shape)\n",
    "\n",
    "    # 3) DataLoader\n",
    "    train_ds = SeqDataset(train_scaled, pred_length=pred_len, regular=True)\n",
    "    test_ds  = SeqDataset(test_scaled,  pred_length=pred_len, regular=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=len(test_ds), shuffle=False)\n",
    "\n",
    "    # 4) YAML 사본 & 길이 갱신\n",
    "    configs = copy.deepcopy(base_configs)\n",
    "    for k in ('seq_length','seq_len','max_seq_len','max_len','context_length'):\n",
    "        if k in configs['model']['params']:\n",
    "            configs['model']['params'][k] = win_size\n",
    "    configs['model']['params']['feature_size'] = feat_num\n",
    "    # ── 저장 끄기 ──\n",
    "    configs['solver']['save_cycle'] = 10**9          # 사실상 never\n",
    "    # (필요시 results_folder도 바꿀 수 있음)\n",
    "\n",
    "    # 5) 모델·트레이너\n",
    "    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "    model  = instantiate_from_config(configs['model']).to(device)\n",
    "\n",
    "    # PosEnc 강제 교체(길이 불일치 방지) ― 경로는 print(model)로 확인\n",
    "    from Models.interpretable_diffusion.model_utils import LearnablePositionalEncoding\n",
    "    d_model = configs['model']['params']['d_model']\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'pos_enc'):\n",
    "        model.model.pos_enc = LearnablePositionalEncoding(d_model, max_len=win_size).to(device)\n",
    "\n",
    "    trainer = Trainer(config=configs, args=args,\n",
    "                      model=model, dataloader={'dataloader': train_loader})\n",
    "    # **checkpoint 저장 완전히 차단**\n",
    "    trainer.save = lambda *a, **k: None\n",
    "\n",
    "    # 6) 학습·복원\n",
    "    all_samples = []\n",
    "    for run in range(1, n_runs+1):\n",
    "        print(f\"[seq_len={seq_len}] run {run}/{n_runs}\")\n",
    "        trainer.train()\n",
    "        sample_i, *_ = trainer.restore(\n",
    "            test_loader,\n",
    "            shape=[win_size, feat_num],\n",
    "            coef=1e-2, stepsize=5e-2, sampling_steps=200\n",
    "        )\n",
    "        sample_i = scaler.inverse_transform(\n",
    "            unnormalize_to_zero_to_one(sample_i.reshape(-1, feat_num))\n",
    "        ).reshape(test_scaled.shape)\n",
    "        all_samples.append(sample_i)\n",
    "\n",
    "    merged = np.concatenate(all_samples, axis=0)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = os.path.join(save_dir, f\"merged_seq{seq_len}.pkl\")\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(merged, f)\n",
    "    print(f\"✔ seq_len={seq_len} 완료 → {out_path}  shape={merged.shape}\")\n",
    "\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('/home1/gkrtod35/Diffusion-TS/Data/merged_data_processed_seoul.csv', low_memory=False)\n",
    "    df_short = df[-(8640+24):]\n",
    "    numeric_df_short = (df_short\n",
    "                        .drop(columns=['Idx','date','time','일시'])\n",
    "                        .apply(pd.to_numeric, errors='coerce')\n",
    "                        .to_numpy())\n",
    "\n",
    "    seq_len_list = [720, 2160, 4320, 8640]\n",
    "    pred_len, stride, n_runs = 24, 24, 100\n",
    "    feat_num = numeric_df_short.shape[1]\n",
    "\n",
    "    args         = Args()\n",
    "    base_configs = load_yaml_config(args.config_path)\n",
    "\n",
    "    for seq_len in seq_len_list:\n",
    "        run_pipeline(seq_len, pred_len, stride, n_runs, feat_num,\n",
    "                     numeric_df_short, base_configs, args,\n",
    "                     save_dir='./merged_results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43179710-a5f2-4ee6-9777-eb05b12f2475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.203660: 100%|██████████| 1000/1000 [08:23<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.125931: 100%|██████████| 1000/1000 [08:24<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:59<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.114440: 100%|██████████| 1000/1000 [08:23<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.111192: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.119049: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.101933: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.085282: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.084264: 100%|██████████| 1000/1000 [08:22<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.077747: 100%|██████████| 1000/1000 [08:22<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.086758: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.088316: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step: 100%|██████████| 200/200 [02:58<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] seq=720 run 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.080898: 100%|██████████| 1000/1000 [08:23<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conditional sampling loop time step:  22%|██▏       | 43/200 [01:04<03:18,  1.26s/it]"
     ]
    }
   ],
   "source": [
    "# 사전 정의 : normalize_to_neg_one_to_one, unnormalize_to_zero_to_one,\n",
    "# SeqDataset, Trainer, load_yaml_config, instantiate_from_config, Args\n",
    "import os, copy, pickle, pandas as pd, numpy as np, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def single_run(seq_len, gpu_id):\n",
    "    pred_len, stride, n_runs = 24, 24, 100\n",
    "    win_size = seq_len + pred_len\n",
    "\n",
    "    # ========== 데이터 준비 ==========\n",
    "    if 'numeric_df_short' not in globals():\n",
    "        raise RuntimeError(\"numeric_df_short 변수부터 만드세요!\")\n",
    "    data = numeric_df_short               # shape (T, C)\n",
    "    feat_num = data.shape[1]\n",
    "\n",
    "    seqs = [data[i:i+win_size] for i in range(0, len(data)-win_size+1, stride)]\n",
    "    seqs = np.stack(seqs, axis=0)\n",
    "    n_train = int(len(seqs)*0.8)\n",
    "    train, test = seqs[:n_train], seqs[n_train:]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.fit_transform(train.reshape(-1, feat_num))\n",
    "    ).reshape(train.shape)\n",
    "    test_scaled = normalize_to_neg_one_to_one(\n",
    "        scaler.transform(test.reshape(-1, feat_num))\n",
    "    ).reshape(test.shape)\n",
    "\n",
    "    train_ds = SeqDataset(train_scaled, pred_length=pred_len, regular=True)\n",
    "    test_ds  = SeqDataset(test_scaled,  pred_length=pred_len, regular=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=len(test_ds), shuffle=False)\n",
    "\n",
    "    # ========== 모델 ==========\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    base_cfg = load_yaml_config(Args().config_path)\n",
    "    cfg = copy.deepcopy(base_cfg)\n",
    "    for k in ('seq_length','seq_len','max_seq_len','max_len','context_length'):\n",
    "        if k in cfg['model']['params']:\n",
    "            cfg['model']['params'][k] = win_size\n",
    "    cfg['model']['params']['feature_size'] = feat_num\n",
    "    cfg['solver']['save_cycle'] = 10**9   # 체크포인트 끔\n",
    "\n",
    "    model = instantiate_from_config(cfg['model']).to(device)\n",
    "    # PosEnc 길이 강제\n",
    "    from Models.interpretable_diffusion.model_utils import LearnablePositionalEncoding\n",
    "    d_model = cfg['model']['params']['d_model']\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'pos_enc'):\n",
    "        model.model.pos_enc = LearnablePositionalEncoding(d_model, max_len=win_size).to(device)\n",
    "\n",
    "    trainer = Trainer(config=cfg, args=Args(), model=model,\n",
    "                      dataloader={'dataloader': train_loader})\n",
    "    trainer.save = lambda *a,**k: None\n",
    "\n",
    "    # ========== 학습/복원 ==========\n",
    "    merged = []\n",
    "    for r in range(1, n_runs+1):\n",
    "        print(f\"[GPU{gpu_id}] seq={seq_len} run {r}/{n_runs}\")\n",
    "        trainer.train()\n",
    "        samp, *_ = trainer.restore(\n",
    "            test_loader, shape=[win_size, feat_num],\n",
    "            coef=1e-2, stepsize=5e-2, sampling_steps=200\n",
    "        )\n",
    "        samp = scaler.inverse_transform(\n",
    "            unnormalize_to_zero_to_one(samp.reshape(-1, feat_num))\n",
    "        ).reshape(test_scaled.shape)\n",
    "        merged.append(samp)\n",
    "    merged = np.concatenate(merged, axis=0)\n",
    "\n",
    "    out_dir = './merged_results'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(f\"{out_dir}/merged_seq{seq_len}.pkl\", 'wb') as f:\n",
    "        pickle.dump(merged, f)\n",
    "    print(f\"✅ GPU{gpu_id} seq_len={seq_len} 끝  shape={merged.shape}\")\n",
    "\n",
    "# -------------------- 실제 실행 --------------------\n",
    "df = pd.read_csv('/home1/gkrtod35/Diffusion-TS/Data/merged_data_processed_seoul.csv',\n",
    "                 low_memory=False)\n",
    "df_short = df[-(8640+24):]\n",
    "numeric_df_short = (df_short\n",
    "                    .drop(columns=['Idx','date','time','일시'])\n",
    "                    .apply(pd.to_numeric, errors='coerce')\n",
    "                    .to_numpy())\n",
    "\n",
    "gpu_list     = list(range(torch.cuda.device_count()))  # [0,1,2,3…]\n",
    "seq_len_list = [720, 2160, 4320, 8640]\n",
    "\n",
    "for i, seq in enumerate(seq_len_list):\n",
    "    gid = gpu_list[i % len(gpu_list)]\n",
    "    single_run(seq, gid)            # 순차 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e84012-8f08-470c-8369-8089f1d3954a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 병렬처리\n",
    "제거 하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ca5fd-fbae-4971-b75d-1f909ec3b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행 매핑: {\n",
      "  \"4320\": 0,\n",
      "  \"8640\": 1\n",
      "}\n",
      "[05:29:09] launch seq=4320 on GPU0\n",
      "[05:29:09] launch seq=8640 on GPU1\n",
      "★ 모든 작업 종료 (로그: logs/diff_*.out , 결과: merged_results/)\n"
     ]
    }
   ],
   "source": [
    "# 5) 여러 길이를 GPU별 subprocess 로 병렬 실행\n",
    "import subprocess, datetime as dt, json, pathlib, sys, torch, os\n",
    "\n",
    "seq_len_list = [4320, 8640]          # 실험할 길이\n",
    "gpu_list     = list(range(torch.cuda.device_count()))\n",
    "if not gpu_list:\n",
    "    raise RuntimeError(\"CUDA GPU가 보이지 않습니다!\")\n",
    "\n",
    "# round-robin 매핑: GPU가 2장뿐이어도 자동 분배\n",
    "seq_gpu = {seq: gpu_list[i % len(gpu_list)] for i, seq in enumerate(seq_len_list)}\n",
    "print(\"실행 매핑:\", json.dumps(seq_gpu, indent=2))\n",
    "\n",
    "log_dir = pathlib.Path(\"logs\"); log_dir.mkdir(exist_ok=True)\n",
    "procs = []\n",
    "\n",
    "for seq, gid in seq_gpu.items():\n",
    "    env = os.environ.copy()\n",
    "    env[\"CUDA_VISIBLE_DEVICES\"] = str(gid)      # ★ 프로세스가 자신 GPU만 보게\n",
    "    log = open(log_dir / f\"diff_{seq}.out\", \"w\")\n",
    "\n",
    "    # 현재 노트북 셀 안 코드를 그대로 실행하려면 python -c 로 호출\n",
    "    # ⇒ 가장 간단: diffusion_run 을 import 후 곧바로 호출\n",
    "    cmd = [\n",
    "        sys.executable, \"- <<END\",\n",
    "        \"import runpy, sys, importlib, pickle, gc, torch, os;\",\n",
    "        \"from __main__ import diffusion_run;\",\n",
    "        f\"diffusion_run({seq}, gpu_id=0)\",\n",
    "        \"END\"\n",
    "    ]\n",
    "    # 위처럼 -c 블록은 문자열 인식 문제가 있으므로 방법2: 파일 저장 후 실행\n",
    "    # 여기서는 runpy 방식 대신 single_run.py(앞서 작성) 경로를 호출하는 편이 안전\n",
    "    cmd = [sys.executable, \"single_run.py\", f\"--seq_len={seq}\", f\"--gpu={gid}\"]\n",
    "\n",
    "    print(f\"[{dt.datetime.now():%H:%M:%S}] launch seq={seq} on GPU{gid}\")\n",
    "    procs.append(subprocess.Popen(cmd, env=env,\n",
    "                                  stdout=log, stderr=subprocess.STDOUT))\n",
    "\n",
    "# 모든 프로세스 종료 대기\n",
    "for p in procs:\n",
    "    p.wait()\n",
    "\n",
    "print(\"★ 모든 작업 종료 (로그: logs/diff_*.out , 결과: merged_results/)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6776074f-257a-4b37-9786-3515b80141b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 'array.pkl'에 저장된 NumPy 배열을 불러오기\n",
    "with open('/home1/gkrtod35/Diffusion-TS/merged_results/merged_seq8640.pkl', 'rb') as f:\n",
    "    merged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09580afb-a81d-4387-8967-4bb5abbdcb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.13304804e-01,\n",
       "       1.97944890e+00, 5.39680798e+00, 8.22719600e+00, 8.86804999e+00,\n",
       "       8.08676004e+00, 7.47905601e+00, 5.28533205e+00, 2.41976789e+00,\n",
       "       5.88211054e-01, 6.82885917e-04, 4.89600802e-02, 1.04760041e-01,\n",
       "       1.07879867e-01, 7.13999403e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.69101984e-02, 1.31736366e-02, 4.35987263e-02, 9.22769200e-02,\n",
       "       9.49173275e-02, 9.59999412e-02, 4.82462504e-02, 3.32982125e-02,\n",
       "       1.31101379e-01, 4.49405484e-01, 1.41408633e+00, 2.26448446e+00,\n",
       "       3.69052151e+00, 5.12492549e+00, 6.85903925e+00, 6.37114927e+00,\n",
       "       3.74832498e+00, 1.38520335e+00, 8.05050859e-02, 0.00000000e+00,\n",
       "       1.68479104e-02, 0.00000000e+00, 0.00000000e+00, 9.68273734e-02])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[10000,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbaf5631-b8ea-4073-a32d-87306908e351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 145, in <module>\n",
      "    single_run(seq_len=args.seq_len, gpu_id=args.gpu)\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 80, in single_run\n",
      "    tr_s = normalize_to_neg_one_to_one(scaler.fit_transform(tr.reshape(-1,feat_num))).reshape(tr.shape)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 892, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 454, in fit\n",
      "    return self.partial_fit(X, y)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 494, in partial_fit\n",
      "    X = validate_data(\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2954, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1128, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required by MinMaxScaler.\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 145, in <module>\n",
      "    single_run(seq_len=args.seq_len, gpu_id=args.gpu)\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 80, in single_run\n",
      "    tr_s = normalize_to_neg_one_to_one(scaler.fit_transform(tr.reshape(-1,feat_num))).reshape(tr.shape)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 892, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 454, in fit\n",
      "    return self.partial_fit(X, y)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 494, in partial_fit\n",
      "    X = validate_data(\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2954, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1128, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required by MinMaxScaler.\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 145, in <module>\n",
      "    single_run(seq_len=args.seq_len, gpu_id=args.gpu)\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 80, in single_run\n",
      "    tr_s = normalize_to_neg_one_to_one(scaler.fit_transform(tr.reshape(-1,feat_num))).reshape(tr.shape)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 892, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 454, in fit\n",
      "    return self.partial_fit(X, y)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 494, in partial_fit\n",
      "    X = validate_data(\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2954, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1128, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required by MinMaxScaler.\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 145, in <module>\n",
      "    single_run(seq_len=args.seq_len, gpu_id=args.gpu)\n",
      "  File \"/gpfs/home1/gkrtod35/Diffusion-TS/single_run.py\", line 80, in single_run\n",
      "    tr_s = normalize_to_neg_one_to_one(scaler.fit_transform(tr.reshape(-1,feat_num))).reshape(tr.shape)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 892, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 454, in fit\n",
      "    return self.partial_fit(X, y)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/base.py\", line 1363, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 494, in partial_fit\n",
      "    X = validate_data(\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2954, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"/home1/gkrtod35/miniconda3/envs/df/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1128, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 19)) while a minimum of 1 is required by MinMaxScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★ 모든 작업 종료\n"
     ]
    }
   ],
   "source": [
    "import subprocess, os, sys\n",
    "\n",
    "# seq_len → GPU 매핑\n",
    "seq_gpu = {720:0, 2160:1, 4320:2, 8640:3}\n",
    "\n",
    "procs = []\n",
    "for seq, gid in seq_gpu.items():\n",
    "    env = os.environ.copy()\n",
    "    env[\"CUDA_VISIBLE_DEVICES\"] = str(gid)          # 각 프로세스가 자기 GPU만 보게\n",
    "    cmd = [sys.executable, \"single_run.py\",\n",
    "           f\"--seq_len={seq}\", f\"--gpu={gid}\"]      # 내부에서 gpu=gid 사용\n",
    "    procs.append(subprocess.Popen(cmd, env=env))\n",
    "\n",
    "for p in procs:\n",
    "    p.wait()\n",
    "\n",
    "print(\"★ 모든 작업 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a99b52-0a07-4e25-9ea2-2e8eb099e412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17a242-ce21-45c5-a560-1dcbb32255e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ad2d7-bc12-466c-a83a-b1da4836e415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab2f7f-713a-41ea-9c23-394e29f32706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fbd53-5591-488e-a571-3f297eafdd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4599146d-4b1e-42cd-a513-262b770ab26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 24, 19)\n",
      "(15, 24)\n"
     ]
    }
   ],
   "source": [
    "# ─── 6) 최종 X_synth, Y_synth 추출 ───\n",
    "X_synth = merged[:, :seq_len, :]       # (n_test, 24, C)\n",
    "Y_synth = merged[:, -pred_len:, 0]     # (n_test, 24)\n",
    "print(X_synth.shape)\n",
    "print(Y_synth.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8575f7cb-6bc6-42be-902f-a0fce8db2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94434ff0-7640-4330-b898-1755b0ab8492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0381d-1381-4adb-a184-ea6d69138176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53b3c0-7299-428c-931a-593e6c676878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270c4eb-d874-48a0-a87f-a1a894442a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
