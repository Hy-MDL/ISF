{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc27eefd-e550-4033-b2f0-3e8358a6f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77966a22-01d7-4e65-addc-83457a358b15",
   "metadata": {},
   "source": [
    "## just forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1774e24c-be5d-418e-b7a3-53a9eefd78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home1/gkrtod35/ISF/TimeGAN/Origin_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f727ee1-6978-4335-9479-7edb345de46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idx</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>solar generation</th>\n",
       "      <th>일시</th>\n",
       "      <th>기온(°C)</th>\n",
       "      <th>풍속(m/s)</th>\n",
       "      <th>풍향(16방위)</th>\n",
       "      <th>습도(%)</th>\n",
       "      <th>증기압(hPa)</th>\n",
       "      <th>...</th>\n",
       "      <th>일조(hr)</th>\n",
       "      <th>일사(MJ/m2)</th>\n",
       "      <th>적설(cm)</th>\n",
       "      <th>전운량(10분위)</th>\n",
       "      <th>중하층운량(10분위)</th>\n",
       "      <th>지면온도(°C)</th>\n",
       "      <th>5cm 지중온도(°C)</th>\n",
       "      <th>10cm 지중온도(°C)</th>\n",
       "      <th>20cm 지중온도(°C)</th>\n",
       "      <th>30cm 지중온도(°C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 00:00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>250.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 01:00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>250.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 02:00</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>250.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 03:00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>250.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-01-01 04:00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>270.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Idx        date  time  solar generation                일시  기온(°C)  풍속(m/s)  \\\n",
       "0    0  2014-01-01     0               0.0  2014-01-01 00:00     3.3      3.8   \n",
       "1    0  2014-01-01     1               0.0  2014-01-01 01:00     2.6      2.3   \n",
       "2    0  2014-01-01     2               0.0  2014-01-01 02:00     1.7      1.7   \n",
       "3    0  2014-01-01     3               0.0  2014-01-01 03:00     1.4      1.4   \n",
       "4    0  2014-01-01     4               0.0  2014-01-01 04:00     0.9      2.8   \n",
       "\n",
       "   풍향(16방위)  습도(%)  증기압(hPa)  ...  일조(hr)  일사(MJ/m2)  적설(cm)  전운량(10분위)  \\\n",
       "0     250.0   65.0       5.0  ...     0.0        0.0     0.0        6.0   \n",
       "1     250.0   66.0       4.9  ...     0.0        0.0     0.0        0.0   \n",
       "2     250.0   67.0       4.6  ...     0.0        0.0     0.0        0.0   \n",
       "3     250.0   60.0       4.1  ...     0.0        0.0     0.0        0.0   \n",
       "4     270.0   59.0       3.8  ...     0.0        0.0     0.0        0.0   \n",
       "\n",
       "   중하층운량(10분위)  지면온도(°C)  5cm 지중온도(°C)  10cm 지중온도(°C)  20cm 지중온도(°C)  \\\n",
       "0          6.0       0.0           0.1           -0.2            0.0   \n",
       "1          0.0      -0.1           0.1           -0.2            0.1   \n",
       "2          0.0      -0.3           0.0           -0.2            0.0   \n",
       "3          0.0      -0.4           0.0           -0.2            0.1   \n",
       "4          0.0      -0.6           0.0           -0.2            0.0   \n",
       "\n",
       "   30cm 지중온도(°C)  \n",
       "0            1.5  \n",
       "1            1.5  \n",
       "2            1.5  \n",
       "3            1.5  \n",
       "4            1.5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('merged_data_processed_seoul.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9527075-35e5-4c3b-8d5c-bd2e91f2fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "seq_len  =  720   # 학습용: 1년치 (1h 단위 → 8760h)\n",
    "pred_len =   24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df = df[-(seq_len + pred_len) : ]  # shape (8760, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b2b9466-ae23-4f08-9c1c-c71571d3c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df = numeric_df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "650226c8-e01b-469d-b129-07a0f90466d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=720, pred_len=24, stride=24):\n",
    "        \"\"\"\n",
    "        data: array-like [T, C]\n",
    "        seq_len: 입력 길이 (예: 720)\n",
    "        pred_len: 예측 길이 (예: 24)\n",
    "        stride: 윈도우 이동 간격 (예: 1)\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        T, C  = data.shape\n",
    "        self.X, self.Y = [], []\n",
    "        for i in range(0, T - seq_len - pred_len + 1, stride):\n",
    "            self.X.append(data[i : i+seq_len])                          # [seq_len, C]\n",
    "            self.Y.append(data[i+seq_len : i+seq_len+pred_len, 0])      # [pred_len]\n",
    "        self.X = torch.from_numpy(np.stack(self.X))  # [N, seq_len, C]\n",
    "        self.Y = torch.from_numpy(np.stack(self.Y))  # [N, pred_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # [C, seq_len], [pred_len]\n",
    "        return self.X[idx].transpose(0,1), self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2370a083-5c35-45d2-bac6-544a60b90b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의: sliding window로 (X, y) 생성\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=8760, pred_len=24):\n",
    "        \"\"\"\n",
    "        data: NumPy array of shape [T, C] (시간 T, 채널 C)\n",
    "        seq_len: 입력 시퀀스 길이 (8760)\n",
    "        pred_len: 예측할 시퀀스 길이 (24)\n",
    "        \"\"\"\n",
    "        data = np.asarray(data, dtype=np.float32)\n",
    "        \n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        T, C = data.shape\n",
    "        for i in range(T - seq_len - pred_len + 1):\n",
    "            self.X.append(data[i:i+seq_len])\n",
    "            self.Y.append(data[i+seq_len:i+seq_len+pred_len, 0])  # 채널0=solar 예측\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)      # [N, seq_len, C]\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.float32)      # [N, pred_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 모델에 넣기 편하게 채널 차원을 앞쪽으로 옮김: [C, seq_len]\n",
    "        return self.X[idx].transpose(0,1), self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad2d633-1f57-4e7d-b03a-fcc38044649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding (sinusoidal)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)              # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)                        # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a5adf96-cc6b-4c08-a7f8-d663879505b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN_LSTM_Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 cnn_channels=32,\n",
    "                 lstm_hidden=64,\n",
    "                 pred_len=24,\n",
    "                 nhead=4,\n",
    "                 num_tf_layers=2,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1D CNN\n",
    "        self.conv1 = nn.Conv1d(in_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.pool  = nn.MaxPool1d(2)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_channels,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Positional Encoding for Transformer\n",
    "        self.pos_enc = PositionalEncoding(d_model=lstm_hidden)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=lstm_hidden,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=lstm_hidden*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_tf_layers)\n",
    "\n",
    "        # Final prediction head\n",
    "        self.fc = nn.Linear(lstm_hidden, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, seq_len]\n",
    "        x = self.relu(self.conv1(x))      # → [B, 16, L]\n",
    "        x = self.relu(self.conv2(x))      # → [B, cnn_channels, L]\n",
    "        x = self.pool(x)                  # → [B, cnn_channels, L/2]\n",
    "\n",
    "        # LSTM expects [B, T, F]\n",
    "        x = x.transpose(1, 2)             # → [B, L/2, cnn_channels]\n",
    "        lstm_out, _ = self.lstm(x)        # → [B, L/2, lstm_hidden]\n",
    "\n",
    "        # Add positional encoding\n",
    "        pe_out = self.pos_enc(lstm_out)   # → [B, L/2, lstm_hidden]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        tf_out = self.transformer(pe_out)  # → [B, L/2, lstm_hidden]\n",
    "\n",
    "        # 마지막 타임스텝만 뽑아서 예측\n",
    "        last = tf_out[:, -1, :]           # → [B, lstm_hidden]\n",
    "        y_hat = self.fc(last)             # → [B, pred_len]\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4f5c0e4-14f7-492e-9083-e084881f187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습·평가 함수 정의\n",
    "def train_epoch(model, loader, optim, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            total_loss += criterion(model(x), y).item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dac56e35-9446-4fa8-84cd-ec73a46a9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: 0.1745 | Val: 0.0510\n",
      "Epoch 02 | Train: 0.0621 | Val: 0.0528\n",
      "Epoch 03 | Train: 0.0543 | Val: 0.0409\n",
      "Epoch 04 | Train: 0.0411 | Val: 0.0310\n",
      "Epoch 05 | Train: 0.0345 | Val: 0.0238\n",
      "Epoch 06 | Train: 0.0325 | Val: 0.0223\n",
      "Epoch 07 | Train: 0.0329 | Val: 0.0208\n",
      "Epoch 08 | Train: 0.0299 | Val: 0.0210\n",
      "Epoch 09 | Train: 0.0296 | Val: 0.0209\n",
      "Epoch 10 | Train: 0.0303 | Val: 0.0227\n",
      "Epoch 11 | Train: 0.0258 | Val: 0.0219\n",
      "Epoch 12 | Train: 0.0260 | Val: 0.0226\n",
      "Epoch 13 | Train: 0.0257 | Val: 0.0221\n",
      "Epoch 14 | Train: 0.0257 | Val: 0.0191\n",
      "Epoch 15 | Train: 0.0253 | Val: 0.0179\n",
      "Epoch 16 | Train: 0.0252 | Val: 0.0182\n",
      "Epoch 17 | Train: 0.0248 | Val: 0.0222\n",
      "Epoch 18 | Train: 0.0262 | Val: 0.0207\n",
      "Epoch 19 | Train: 0.0248 | Val: 0.0198\n",
      "Epoch 20 | Train: 0.0244 | Val: 0.0209\n",
      "Epoch 21 | Train: 0.0245 | Val: 0.0184\n",
      "Epoch 22 | Train: 0.0237 | Val: 0.0205\n",
      "Epoch 23 | Train: 0.0226 | Val: 0.0205\n",
      "Epoch 24 | Train: 0.0231 | Val: 0.0197\n",
      "Epoch 25 | Train: 0.0228 | Val: 0.0196\n",
      "Epoch 26 | Train: 0.0207 | Val: 0.0211\n",
      "Epoch 27 | Train: 0.0219 | Val: 0.0222\n",
      "Epoch 28 | Train: 0.0209 | Val: 0.0211\n",
      "Epoch 29 | Train: 0.0199 | Val: 0.0197\n",
      "Epoch 30 | Train: 0.0216 | Val: 0.0183\n",
      "Epoch 31 | Train: 0.0180 | Val: 0.0233\n",
      "Epoch 32 | Train: 0.0179 | Val: 0.0219\n",
      "Epoch 33 | Train: 0.0162 | Val: 0.0192\n",
      "Epoch 34 | Train: 0.0164 | Val: 0.0244\n",
      "Epoch 35 | Train: 0.0142 | Val: 0.0243\n",
      "Epoch 36 | Train: 0.0129 | Val: 0.0270\n",
      "Epoch 37 | Train: 0.0141 | Val: 0.0285\n",
      "Epoch 38 | Train: 0.0121 | Val: 0.0340\n",
      "Epoch 39 | Train: 0.0152 | Val: 0.0239\n",
      "Epoch 40 | Train: 0.0158 | Val: 0.0288\n",
      "Epoch 41 | Train: 0.0134 | Val: 0.0324\n",
      "Epoch 42 | Train: 0.0158 | Val: 0.0203\n",
      "Epoch 43 | Train: 0.0112 | Val: 0.0297\n",
      "Epoch 44 | Train: 0.0121 | Val: 0.0288\n",
      "Epoch 45 | Train: 0.0095 | Val: 0.0274\n",
      "Epoch 46 | Train: 0.0109 | Val: 0.0331\n",
      "Epoch 47 | Train: 0.0111 | Val: 0.0332\n",
      "Epoch 48 | Train: 0.0096 | Val: 0.0304\n",
      "Epoch 49 | Train: 0.0092 | Val: 0.0302\n",
      "Epoch 50 | Train: 0.0086 | Val: 0.0307\n",
      "Epoch 51 | Train: 0.0084 | Val: 0.0287\n",
      "Epoch 52 | Train: 0.0081 | Val: 0.0283\n",
      "Epoch 53 | Train: 0.0092 | Val: 0.0326\n",
      "Epoch 54 | Train: 0.0089 | Val: 0.0289\n",
      "Epoch 55 | Train: 0.0089 | Val: 0.0290\n",
      "Epoch 56 | Train: 0.0088 | Val: 0.0321\n",
      "Epoch 57 | Train: 0.0069 | Val: 0.0277\n",
      "Epoch 58 | Train: 0.0073 | Val: 0.0328\n",
      "Epoch 59 | Train: 0.0067 | Val: 0.0306\n",
      "Epoch 60 | Train: 0.0067 | Val: 0.0291\n",
      "Epoch 61 | Train: 0.0077 | Val: 0.0313\n",
      "Epoch 62 | Train: 0.0059 | Val: 0.0255\n",
      "Epoch 63 | Train: 0.0065 | Val: 0.0285\n",
      "Epoch 64 | Train: 0.0058 | Val: 0.0284\n",
      "Epoch 65 | Train: 0.0059 | Val: 0.0294\n",
      "Epoch 66 | Train: 0.0061 | Val: 0.0312\n",
      "Epoch 67 | Train: 0.0057 | Val: 0.0296\n",
      "Epoch 68 | Train: 0.0047 | Val: 0.0308\n",
      "Epoch 69 | Train: 0.0058 | Val: 0.0298\n",
      "Epoch 70 | Train: 0.0055 | Val: 0.0303\n",
      "Epoch 71 | Train: 0.0057 | Val: 0.0285\n",
      "Epoch 72 | Train: 0.0065 | Val: 0.0305\n",
      "Epoch 73 | Train: 0.0048 | Val: 0.0282\n",
      "Epoch 74 | Train: 0.0054 | Val: 0.0296\n",
      "Epoch 75 | Train: 0.0047 | Val: 0.0273\n",
      "Epoch 76 | Train: 0.0042 | Val: 0.0299\n",
      "Epoch 77 | Train: 0.0047 | Val: 0.0290\n",
      "Epoch 78 | Train: 0.0042 | Val: 0.0295\n",
      "Epoch 79 | Train: 0.0040 | Val: 0.0299\n",
      "Epoch 80 | Train: 0.0044 | Val: 0.0297\n",
      "Epoch 81 | Train: 0.0045 | Val: 0.0308\n",
      "Epoch 82 | Train: 0.0045 | Val: 0.0288\n",
      "Epoch 83 | Train: 0.0047 | Val: 0.0289\n",
      "Epoch 84 | Train: 0.0042 | Val: 0.0286\n",
      "Epoch 85 | Train: 0.0045 | Val: 0.0263\n",
      "Epoch 86 | Train: 0.0047 | Val: 0.0279\n",
      "Epoch 87 | Train: 0.0050 | Val: 0.0253\n",
      "Epoch 88 | Train: 0.0050 | Val: 0.0306\n",
      "Epoch 89 | Train: 0.0036 | Val: 0.0296\n",
      "Epoch 90 | Train: 0.0043 | Val: 0.0331\n",
      "Epoch 91 | Train: 0.0048 | Val: 0.0255\n",
      "Epoch 92 | Train: 0.0040 | Val: 0.0270\n",
      "Epoch 93 | Train: 0.0034 | Val: 0.0276\n",
      "Epoch 94 | Train: 0.0037 | Val: 0.0267\n",
      "Epoch 95 | Train: 0.0034 | Val: 0.0294\n",
      "Epoch 96 | Train: 0.0031 | Val: 0.0266\n",
      "Epoch 97 | Train: 0.0037 | Val: 0.0284\n",
      "Epoch 98 | Train: 0.0040 | Val: 0.0280\n",
      "Epoch 99 | Train: 0.0039 | Val: 0.0268\n",
      "Epoch 100 | Train: 0.0030 | Val: 0.0314\n"
     ]
    }
   ],
   "source": [
    "# 셀 6: 실행 예시\n",
    "import numpy as np\n",
    "if __name__ == \"__main__\":\n",
    "    data = numeric_df.values                                       # [T, C]\n",
    "\n",
    "    # 2) 정규화\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    # 3) 데이터셋·로더 준비\n",
    "    #seq_len, pred_len = 8760, 24\n",
    "    seq_len, pred_len = 24, 24\n",
    "    #ds      = TimeSeriesDataset(data, seq_len, pred_len)\n",
    "    ds = SlidingWindowDataset(data, seq_len, pred_len, stride=24)\n",
    "    n_train = int(len(ds)*0.8)\n",
    "    n_val   = len(ds) - n_train\n",
    "    train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n_val])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "    # 4) 모델·옵티마이저·손실함수\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model   = CNN_LSTM_Transformer(in_channels=data.shape[1]).to(device)\n",
    "    optim   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 5) 학습 루프\n",
    "    epochs = 100\n",
    "    for epoch in range(1, epochs+1):\n",
    "        tr_loss  = train_epoch(model, train_loader, optim, criterion, device)\n",
    "        val_loss = eval_epoch( model, val_loader,   criterion, device)\n",
    "        print(f\"Epoch {epoch:02d} | Train: {tr_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "    # 6) 샘플 예측\n",
    "    x_sample, y_true = ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_scaled = y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fde087f-caa0-4c70-9640-2f6608fd84ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True next-24h (원단위): [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 2.0600000e-04 1.1517700e-01 1.1016750e+00 9.3735301e-01\n",
      " 1.7527210e+00 1.5623450e+00 1.6476680e+00 1.5117821e+00 1.4694040e+00\n",
      " 6.6337299e-01 3.8325500e-01 1.1097999e-02 2.0080000e-02 1.2880000e-02\n",
      " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "Pred next-24h (원단위): [-0.09527825  0.02718289 -0.12057758 -0.03576751 -0.24859226  0.3356997\n",
      "  0.17173001 -0.09421639  1.1735973   3.3381796   4.8104835   4.205619\n",
      "  4.509196    4.3904595   3.1885471   1.2005687   0.6886464  -0.11406687\n",
      " -0.16844492  0.18786845  0.02426887 -0.32298997  0.1899375  -0.37093845]\n"
     ]
    }
   ],
   "source": [
    "# --- 역스케일링 시작 ---\n",
    "import numpy as np\n",
    "solar_idx = 0\n",
    "\n",
    "# 전체 피처 개수\n",
    "C = data.shape[1]\n",
    "\n",
    "# 예측/실제 배열을 (pred_len, C) 모양으로 만들고 \n",
    "pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "\n",
    "# solar generation 채널(solar_idx)에만 값 채우기\n",
    "pred_full[:, solar_idx] = y_pred_scaled\n",
    "true_full[:, solar_idx] = y_true_scaled\n",
    "\n",
    "# MinMaxScaler.inverse_transform\n",
    "pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "# --- 역스케일링 끝 ---\n",
    "\n",
    "print(\"True next-24h (원단위):\", true_orig)\n",
    "print(\"Pred next-24h (원단위):\", pred_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80e778b4-768f-4526-b583-73420a37f843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3315\n"
     ]
    }
   ],
   "source": [
    "# --- RMSE 계산 ---\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(np.mean((pred_orig - true_orig) ** 2))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11f351-7341-44f2-b5a5-bcbb0789cb6c",
   "metadata": {},
   "source": [
    "## 보름치 데이터\n",
    "보름치 데이터로 하루 예측한 RMSE와\n",
    "\n",
    "디퓨전 생성모형을 통해 보름치 데이터를 여러개 만들어 넣은 걸 합친 데이터로 하루 예측한 RMSE로 \n",
    "\n",
    "두 가지 경우를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12246b4f-54cd-4c47-b5e3-568c0978998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df = pd.read_csv('merged_data_processed_seoul.csv', low_memory=False)\n",
    "\n",
    "#seq_len  = 360   # 학습용: 보름치 (1h 단위 → 360h)\n",
    "seq_len = 720\n",
    "pred_len =  24   # 테스트용: 1일치 (24h)\n",
    "\n",
    "# 꼬리(tail)에서 잘라내기\n",
    "#    – train_data: 마지막(pred_len)시간 바로 앞의 seq_len시간\n",
    "#    – test_data : 마지막 pred_len시간\n",
    "df_short = df[-(seq_len + pred_len) : ]  # \n",
    "numeric_df_short = df_short.drop(columns=['Idx','date','time','일시'])\n",
    "numeric_df_short = numeric_df_short.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a9864-b65e-4eff-9491-acb5e1c63350",
   "metadata": {},
   "source": [
    "### 보름치 데이터로 하루 예측한 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ed0cb-b790-4178-8f29-744e05dfcbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4420\n",
      "[001/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[002/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[003/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[004/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[005/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[006/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[007/100] RMSE = 0.4420\n",
      "RMSE: 0.4420\n",
      "[008/100] RMSE = 0.4420\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, random, gc\n",
    "\n",
    "NUM_RUNS = 100\n",
    "RMSEs    = []\n",
    "\n",
    "for run in range(1, NUM_RUNS+1):\n",
    "\n",
    "    # ── 1) 랜덤시드 고정 (run 값으로 변주) ────────────\n",
    "    seed = 42 + run\n",
    "    torch.manual_seed(seed);   np.random.seed(seed);   random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "    import numpy as np\n",
    "    if __name__ == \"__main__\":\n",
    "        data = numeric_df_short.values                                       # [T, C]\n",
    "\n",
    "        # 정규화\n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "        # 데이터셋·로더 준비\n",
    "        #seq_len, pred_len = 8760, 24\n",
    "        seq_len, pred_len = 24, 24\n",
    "        #ds      = TimeSeriesDataset(data, seq_len, pred_len)\n",
    "        ds = SlidingWindowDataset(data, seq_len, pred_len, stride=24)\n",
    "        n_train = int(len(ds)*0.8)\n",
    "        n_val   = len(ds) - n_train\n",
    "        train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n_val])\n",
    "        train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "        # 모델·옵티마이저·손실함수\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model   = CNN_LSTM_Transformer(in_channels=data.shape[1]).to(device)\n",
    "        optim   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # 학습 루프\n",
    "        epochs = 100\n",
    "        for epoch in range(1, epochs+1):\n",
    "            tr_loss  = train_epoch(model, train_loader, optim, criterion, device)\n",
    "            val_loss = eval_epoch( model, val_loader,   criterion, device)\n",
    "            #print(f\"Epoch {epoch:02d} | Train: {tr_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        # 샘플 예측\n",
    "        x_sample, y_true = ds[0]\n",
    "        x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "        y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "        y_true_scaled = y_true.numpy()\n",
    "\n",
    "    # --- 역스케일링 시작 ---\n",
    "    solar_idx = 0\n",
    "\n",
    "    # 전체 피처 개수\n",
    "    C = data.shape[1]\n",
    "\n",
    "    # 예측/실제 배열을 (pred_len, C) 모양으로 만들고 \n",
    "    pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "\n",
    "    # solar generation 채널(solar_idx)에만 값 채우기\n",
    "    pred_full[:, solar_idx] = y_pred_scaled\n",
    "    true_full[:, solar_idx] = y_true_scaled\n",
    "\n",
    "    # MinMaxScaler.inverse_transform\n",
    "    pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "    true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "\n",
    "    # --- RMSE 계산 ---\n",
    "    rmse = np.sqrt(np.mean((pred_orig - true_orig) ** 2))\n",
    "    #print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    RMSEs.append(rmse)       # 결과 저장\n",
    "    print(f\"[{run:03d}/{NUM_RUNS}] RMSE = {rmse:.4f}\")\n",
    "    \n",
    "    # ── 3) GPU 메모리·파이썬 객체 청소 (필수는 아니지만 안전) ──\n",
    "    del model, train_loader, val_loader\n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# ── 4) 통계량 출력 ─────────────────────────────────────\n",
    "RMSEs = np.array(RMSEs)\n",
    "mean  = RMSEs.mean()\n",
    "var   = RMSEs.var(ddof=0)          # 불편분산은 ddof=1\n",
    "std   = RMSEs.std(ddof=0)\n",
    "\n",
    "print(\"\\n========== 100-run Summary ==========\")\n",
    "print(f\"Mean RMSE  : {mean:.4f}\")\n",
    "print(f\"Variance   : {var:.6f}\")\n",
    "print(f\"Std. Dev.  : {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0362f96b-40b8-404c-aecf-c6e146b0c1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: 0.3296 | Val: 0.0634\n",
      "Epoch 02 | Train: 0.0845 | Val: 0.0604\n",
      "Epoch 03 | Train: 0.0775 | Val: 0.0405\n",
      "Epoch 04 | Train: 0.0557 | Val: 0.0291\n",
      "Epoch 05 | Train: 0.0522 | Val: 0.0234\n",
      "Epoch 06 | Train: 0.0431 | Val: 0.0200\n",
      "Epoch 07 | Train: 0.0372 | Val: 0.0182\n",
      "Epoch 08 | Train: 0.0351 | Val: 0.0172\n",
      "Epoch 09 | Train: 0.0320 | Val: 0.0176\n",
      "Epoch 10 | Train: 0.0315 | Val: 0.0159\n",
      "Epoch 11 | Train: 0.0327 | Val: 0.0139\n",
      "Epoch 12 | Train: 0.0292 | Val: 0.0129\n",
      "Epoch 13 | Train: 0.0291 | Val: 0.0129\n",
      "Epoch 14 | Train: 0.0297 | Val: 0.0132\n",
      "Epoch 15 | Train: 0.0284 | Val: 0.0133\n",
      "Epoch 16 | Train: 0.0310 | Val: 0.0126\n",
      "Epoch 17 | Train: 0.0311 | Val: 0.0126\n",
      "Epoch 18 | Train: 0.0294 | Val: 0.0122\n",
      "Epoch 19 | Train: 0.0313 | Val: 0.0127\n",
      "Epoch 20 | Train: 0.0309 | Val: 0.0125\n",
      "Epoch 21 | Train: 0.0281 | Val: 0.0121\n",
      "Epoch 22 | Train: 0.0286 | Val: 0.0129\n",
      "Epoch 23 | Train: 0.0272 | Val: 0.0123\n",
      "Epoch 24 | Train: 0.0304 | Val: 0.0130\n",
      "Epoch 25 | Train: 0.0286 | Val: 0.0121\n",
      "Epoch 26 | Train: 0.0298 | Val: 0.0125\n",
      "Epoch 27 | Train: 0.0289 | Val: 0.0123\n",
      "Epoch 28 | Train: 0.0281 | Val: 0.0130\n",
      "Epoch 29 | Train: 0.0299 | Val: 0.0132\n",
      "Epoch 30 | Train: 0.0289 | Val: 0.0124\n",
      "Epoch 31 | Train: 0.0271 | Val: 0.0117\n",
      "Epoch 32 | Train: 0.0295 | Val: 0.0117\n",
      "Epoch 33 | Train: 0.0267 | Val: 0.0131\n",
      "Epoch 34 | Train: 0.0266 | Val: 0.0131\n",
      "Epoch 35 | Train: 0.0273 | Val: 0.0134\n",
      "Epoch 36 | Train: 0.0257 | Val: 0.0134\n",
      "Epoch 37 | Train: 0.0256 | Val: 0.0132\n",
      "Epoch 38 | Train: 0.0268 | Val: 0.0134\n",
      "Epoch 39 | Train: 0.0277 | Val: 0.0114\n",
      "Epoch 40 | Train: 0.0282 | Val: 0.0124\n",
      "Epoch 41 | Train: 0.0273 | Val: 0.0125\n",
      "Epoch 42 | Train: 0.0248 | Val: 0.0129\n",
      "Epoch 43 | Train: 0.0260 | Val: 0.0131\n",
      "Epoch 44 | Train: 0.0259 | Val: 0.0121\n",
      "Epoch 45 | Train: 0.0253 | Val: 0.0124\n",
      "Epoch 46 | Train: 0.0257 | Val: 0.0126\n",
      "Epoch 47 | Train: 0.0282 | Val: 0.0149\n",
      "Epoch 48 | Train: 0.0246 | Val: 0.0129\n",
      "Epoch 49 | Train: 0.0276 | Val: 0.0120\n",
      "Epoch 50 | Train: 0.0261 | Val: 0.0122\n",
      "Epoch 51 | Train: 0.0263 | Val: 0.0123\n",
      "Epoch 52 | Train: 0.0253 | Val: 0.0110\n",
      "Epoch 53 | Train: 0.0234 | Val: 0.0105\n",
      "Epoch 54 | Train: 0.0230 | Val: 0.0113\n",
      "Epoch 55 | Train: 0.0242 | Val: 0.0126\n",
      "Epoch 56 | Train: 0.0209 | Val: 0.0118\n",
      "Epoch 57 | Train: 0.0224 | Val: 0.0104\n",
      "Epoch 58 | Train: 0.0195 | Val: 0.0115\n",
      "Epoch 59 | Train: 0.0185 | Val: 0.0104\n",
      "Epoch 60 | Train: 0.0196 | Val: 0.0102\n",
      "Epoch 61 | Train: 0.0187 | Val: 0.0113\n",
      "Epoch 62 | Train: 0.0187 | Val: 0.0124\n",
      "Epoch 63 | Train: 0.0179 | Val: 0.0129\n",
      "Epoch 64 | Train: 0.0171 | Val: 0.0136\n",
      "Epoch 65 | Train: 0.0166 | Val: 0.0130\n",
      "Epoch 66 | Train: 0.0183 | Val: 0.0116\n",
      "Epoch 67 | Train: 0.0149 | Val: 0.0151\n",
      "Epoch 68 | Train: 0.0163 | Val: 0.0129\n",
      "Epoch 69 | Train: 0.0165 | Val: 0.0139\n",
      "Epoch 70 | Train: 0.0142 | Val: 0.0158\n",
      "Epoch 71 | Train: 0.0138 | Val: 0.0124\n",
      "Epoch 72 | Train: 0.0139 | Val: 0.0120\n",
      "Epoch 73 | Train: 0.0122 | Val: 0.0148\n",
      "Epoch 74 | Train: 0.0136 | Val: 0.0169\n",
      "Epoch 75 | Train: 0.0113 | Val: 0.0157\n",
      "Epoch 76 | Train: 0.0136 | Val: 0.0171\n",
      "Epoch 77 | Train: 0.0098 | Val: 0.0205\n",
      "Epoch 78 | Train: 0.0121 | Val: 0.0230\n",
      "Epoch 79 | Train: 0.0139 | Val: 0.0133\n",
      "Epoch 80 | Train: 0.0117 | Val: 0.0117\n",
      "Epoch 81 | Train: 0.0109 | Val: 0.0153\n",
      "Epoch 82 | Train: 0.0119 | Val: 0.0197\n",
      "Epoch 83 | Train: 0.0100 | Val: 0.0184\n",
      "Epoch 84 | Train: 0.0097 | Val: 0.0160\n",
      "Epoch 85 | Train: 0.0092 | Val: 0.0204\n",
      "Epoch 86 | Train: 0.0086 | Val: 0.0225\n",
      "Epoch 87 | Train: 0.0078 | Val: 0.0212\n",
      "Epoch 88 | Train: 0.0082 | Val: 0.0184\n",
      "Epoch 89 | Train: 0.0069 | Val: 0.0174\n",
      "Epoch 90 | Train: 0.0069 | Val: 0.0177\n",
      "Epoch 91 | Train: 0.0071 | Val: 0.0209\n",
      "Epoch 92 | Train: 0.0077 | Val: 0.0202\n",
      "Epoch 93 | Train: 0.0065 | Val: 0.0169\n",
      "Epoch 94 | Train: 0.0094 | Val: 0.0166\n",
      "Epoch 95 | Train: 0.0083 | Val: 0.0276\n",
      "Epoch 96 | Train: 0.0077 | Val: 0.0183\n",
      "Epoch 97 | Train: 0.0066 | Val: 0.0206\n",
      "Epoch 98 | Train: 0.0060 | Val: 0.0237\n",
      "Epoch 99 | Train: 0.0058 | Val: 0.0185\n",
      "Epoch 100 | Train: 0.0056 | Val: 0.0219\n",
      "RMSE: 0.4420\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "if __name__ == \"__main__\":\n",
    "    data = numeric_df_short.values                                       # [T, C]\n",
    "\n",
    "    # 정규화\n",
    "    scaler = MinMaxScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "\n",
    "    # 데이터셋·로더 준비\n",
    "    #seq_len, pred_len = 8760, 24\n",
    "    seq_len, pred_len = 24, 24\n",
    "    #ds      = TimeSeriesDataset(data, seq_len, pred_len)\n",
    "    ds = SlidingWindowDataset(data, seq_len, pred_len, stride=24)\n",
    "    n_train = int(len(ds)*0.8)\n",
    "    n_val   = len(ds) - n_train\n",
    "    train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n_val])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "    # 모델·옵티마이저·손실함수\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model   = CNN_LSTM_Transformer(in_channels=data.shape[1]).to(device)\n",
    "    optim   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 학습 루프\n",
    "    epochs = 100\n",
    "    for epoch in range(1, epochs+1):\n",
    "        tr_loss  = train_epoch(model, train_loader, optim, criterion, device)\n",
    "        val_loss = eval_epoch( model, val_loader,   criterion, device)\n",
    "        #print(f\"Epoch {epoch:02d} | Train: {tr_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "    # 샘플 예측\n",
    "    x_sample, y_true = ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_scaled = y_true.numpy()\n",
    "\n",
    "# --- 역스케일링 시작 ---\n",
    "solar_idx = 0\n",
    "\n",
    "# 전체 피처 개수\n",
    "C = data.shape[1]\n",
    "\n",
    "# 예측/실제 배열을 (pred_len, C) 모양으로 만들고 \n",
    "pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "\n",
    "# solar generation 채널(solar_idx)에만 값 채우기\n",
    "pred_full[:, solar_idx] = y_pred_scaled\n",
    "true_full[:, solar_idx] = y_true_scaled\n",
    "\n",
    "# MinMaxScaler.inverse_transform\n",
    "pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "\n",
    "# --- RMSE 계산 ---\n",
    "rmse = np.sqrt(np.mean((pred_orig - true_orig) ** 2))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18fddd9a-9096-498e-9fe0-0e1d07a02be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc14c0-9457-4b11-8f0e-6e6f0f0b6b39",
   "metadata": {},
   "source": [
    "### 디퓨전 생성모형을 통해 보름치 데이터를 여러개 만들어 넣은 걸 합친 데이터로 하루 예측한 RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b2d717a-bab8-4677-a9c7-de81fc883814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 'array.pkl'에 저장된 NumPy 배열을 불러오기\n",
    "with open('/home1/gkrtod35/Diffusion-TS/array.pkl', 'rb') as f:\n",
    "    merged = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4013c6e2-1d92-4302-a5b8-e40bef0cb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성 데이터에서 X_synth, Y_synth 뽑기\n",
    "X_synth = merged[:, :seq_len, :]            # (N_synth, seq_len, C)\n",
    "Y_synth = merged[:, seq_len:seq_len+pred_len, 0]  # (N_synth, pred_len)  # 채널0=타깃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3eefdcb0-71c6-4e21-9b31-9946cd21cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape == (T, C)\n",
    "ds_real = SlidingWindowDataset(data, seq_len=seq_len, pred_len=pred_len, stride=24)\n",
    "# ds_real.X.shape == (N_real, seq_len, C)\n",
    "# ds_real.Y.shape == (N_real, pred_len)\n",
    "\n",
    "X_real = ds_real.X.numpy()\n",
    "Y_real = ds_real.Y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca5c0069-bde9-4e30-84ab-4523503837b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1) 합치기\n",
    "X_all = np.concatenate([X_real, X_synth], axis=0)  # (N_real+N_synth, seq_len, C)\n",
    "Y_all = np.concatenate([Y_real, Y_synth], axis=0)  # (N_real+N_synth, pred_len)\n",
    "\n",
    "# 2) 텐서로 변환하고, 채널 축을 앞쪽으로 옮기기: [N, C, seq_len]\n",
    "X_all = torch.tensor(X_all, dtype=torch.float32).transpose(1,2)\n",
    "Y_all = torch.tensor(Y_all, dtype=torch.float32)\n",
    "\n",
    "# 3) Dataset & DataLoader\n",
    "combined_ds = TensorDataset(X_all, Y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c08c934a-0f1b-42f2-bc2b-217b1014cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: 5.2827 | Val: 3.4641\n",
      "Epoch 02 | Train: 3.2988 | Val: 2.8172\n",
      "Epoch 03 | Train: 2.5350 | Val: 1.9848\n",
      "Epoch 04 | Train: 1.2657 | Val: 0.5921\n",
      "Epoch 05 | Train: 0.6135 | Val: 0.2047\n",
      "Epoch 06 | Train: 0.3278 | Val: 0.0770\n",
      "Epoch 07 | Train: 0.2282 | Val: 0.0499\n",
      "Epoch 08 | Train: 0.2127 | Val: 0.0351\n",
      "Epoch 09 | Train: 0.1940 | Val: 0.0249\n",
      "Epoch 10 | Train: 0.1923 | Val: 0.0325\n",
      "Epoch 11 | Train: 0.1950 | Val: 0.0292\n",
      "Epoch 12 | Train: 0.1878 | Val: 0.1402\n",
      "Epoch 13 | Train: 0.1949 | Val: 0.0251\n",
      "Epoch 14 | Train: 0.1774 | Val: 0.0662\n",
      "Epoch 15 | Train: 0.1741 | Val: 0.0328\n",
      "Epoch 16 | Train: 0.1819 | Val: 0.0237\n",
      "Epoch 17 | Train: 0.1708 | Val: 0.0346\n",
      "Epoch 18 | Train: 0.1774 | Val: 0.0301\n",
      "Epoch 19 | Train: 0.1802 | Val: 0.0245\n",
      "Epoch 20 | Train: 0.1704 | Val: 0.0629\n",
      "Epoch 21 | Train: 0.1705 | Val: 0.0272\n",
      "Epoch 22 | Train: 0.1798 | Val: 0.0304\n",
      "Epoch 23 | Train: 0.1738 | Val: 0.0233\n",
      "Epoch 24 | Train: 0.1783 | Val: 0.0413\n",
      "Epoch 25 | Train: 0.1791 | Val: 0.0368\n",
      "Epoch 26 | Train: 0.1745 | Val: 0.0291\n",
      "Epoch 27 | Train: 0.1873 | Val: 0.0326\n",
      "Epoch 28 | Train: 0.1733 | Val: 0.0250\n",
      "Epoch 29 | Train: 0.1724 | Val: 0.0302\n",
      "Epoch 30 | Train: 0.1718 | Val: 0.0241\n",
      "RMSE: 1.9955\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 5) train/val \n",
    "if __name__ == \"__main__\":\n",
    "    n_train = int(len(combined_ds) * 0.8)\n",
    "    n_val   = len(combined_ds) - n_train\n",
    "    train_ds, val_ds = random_split(combined_ds, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "    # 6) 모델·옵티마이저·손실함수 세팅\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model     = CNN_LSTM_Transformer(in_channels=data.shape[1]).to(device)\n",
    "    optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 7) 학습 루프\n",
    "    epochs = 30\n",
    "    for epoch in range(1, epochs+1):\n",
    "        tr_loss  = train_epoch(model, train_loader, optim, criterion, device)\n",
    "        val_loss = eval_epoch( model, val_loader,   criterion, device)\n",
    "        print(f\"Epoch {epoch:02d} | Train: {tr_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "    # 8) 샘플 예측 + 역스케일링 + RMSE 계산\n",
    "    x_sample, y_true = combined_ds[0]\n",
    "    x_sample = x_sample.unsqueeze(0).to(device)  # [1, C, seq_len]\n",
    "    y_pred_scaled = model(x_sample).cpu().detach().numpy().flatten()\n",
    "    y_true_scaled = y_true.numpy()\n",
    "\n",
    "    # 역스케일링\n",
    "    solar_idx = 0\n",
    "    C = data.shape[1]\n",
    "    pred_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    true_full = np.zeros((pred_len, C), dtype=np.float32)\n",
    "    pred_full[:, solar_idx] = y_pred_scaled\n",
    "    true_full[:, solar_idx] = y_true_scaled\n",
    "    pred_orig = scaler.inverse_transform(pred_full)[:, solar_idx]\n",
    "    true_orig = scaler.inverse_transform(true_full)[:, solar_idx]\n",
    "\n",
    "    rmse = np.sqrt(np.mean((pred_orig - true_orig)**2))\n",
    "    print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e087bf78-e870-4970-859c-f13a0a2e8dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
